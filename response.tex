\begin{center}
	\large
	Response Letter
\end{center}
We thank the reviewers for their useful comments and feedback on improving the manuscript and the presentation. In the following, we summarize changes reflected in the revised manuscript.


\paragraph{Distribution shifts in life-long learning.} We propose Hamming distance heuristics in {\imli} with the assumption that distribution remains same. One way to account for distribution shift is to consider last $ p $ batches instead of the (single) previous batch in the objective function in mini-batch learning. For a feature variable $ B^i_j $, we consider its majority assignment in last $ p $ classification rules and encode as a soft clause to retain the majority assignment in the current batch. Moreover, we can reweigh the soft clause by prioritizing assignments of $ B^i_j $ in recent batches.
%When distribution changes, the classifier $ \mathcal{R}_{i} $ for the $ i^ $ 
\red{[Add page number]}

\paragraph{Discussions of related works in experiments.}{\imli} scales well than existing interpretable rule-based classifiers because of the incremental solving approach. Most existing works rely on rule-mining of potential classification rules followed by an optimization algorithm such as Bayesian optimization and branch and bound algorithm. We observe incrementality of learning to be more effective. We elaborate this in Section~\ref{interpretability_imli_sec:experiments_scalability}, page 39.


\paragraph{How does {\justicia} outperform a direct approach by learning a Bayesian Network on limited samples?} We agree with the reviewer that learning a Bayesian Network on limited samples does not always result in a better robustness (e.g., less standard deviation of fairness metrics estimation) of {\justicia} than the direct approach estimating fairness on a dataset. 

Elaborately, in Chapter~\ref{chapter:justicia} Figure~\ref{fairness_justicia_fig:sample-size}, {\justicia} demonstrates higher robustness than the direct approach, where we consider the distribution in two ways: (i) non-sensitive features are independent and (ii) they are dependent on sensitive features only. Thus, Figure~\ref{fairness_justicia_fig:sample-size} does not involve experiments with Bayesian Network, which we introduce later in Chapter~\ref{chapter:fvgm}. In our revised experiment, we observe that {\justicia} with learning Bayesian network on a dataset, called {\fvgm}, exhibits less robustness due to network learning.


\paragraph{Clarification on Fairness Influence Function (FIF).} We consider an axiom that FIFs are additive where the sum of FIFs of all subsets of non-sensitive features is equal \textit{to the resultant unfairness of the classifier}. Here the unfairness of the classifier is a real value in $ [0,1] $, such as statistical parity, equalized odds, and predictive parity. While one may choose a different axiom, such as multiplicative FIFs, our choice is motivated by the explainability literature for machine learning, such as in SHAP~\cite{begley2020explainability,lundberg2020explaining}. In particular, additivity of FIFs is based on the idea of decomposing the bias/unfairness as the sum of the contribution of different subsets of features on the unfairness of the classifier.



