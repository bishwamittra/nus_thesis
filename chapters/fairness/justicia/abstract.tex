\section{Abstract}
	As a technology machine learning\improvement{Unnecessary abbreviation} is oblivious to societal good or bad, and thus, the field of fair machine learning has stepped up to propose multiple mathematical definitions and algorithms to ensure different notions of fairness in machine learning classifiers.	Given the multitude of propositions, it has become imperative to formally verify the fairness of different classifiers on different datasets.	In this chapter\change{Paper replaced with chapter}, we propose a \textit{stochastic satisfiability} (SSAT) based probabilistic fairness verification framework, called {\justicia}, that formally verifies different fairness measures of supervised classifiers given the distribution of input features. We instantiate {\justicia} on multiple classifiers, bias mitigation algorithms, and fairness datasets to verify different fairness metrics, such as disparate impact, statistical parity, and equalized odds.
	{\justicia} is scalable, accurate, and operates on non-Boolean and compound sensitive features\change{Attributes replaced with features} unlike existing probabilistic verifiers, such as FairSquare and VeriFair.
	Being distribution-based by design, {\justicia} is more robust than verifiers such as AIF360, that operate on specific test samples.
	We also theoretically bound the finite-sample error of the verified fairness measure.
