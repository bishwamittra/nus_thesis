
\section{Practical Settings}
\label{sec:practical-setting}
In this section, we  relax assumptions of Boolean classifiers and Boolean attributes and extend {\framework} to verify fairness metrics in a more practical setting. We first discuss the input classifiers of {\framework} in the following.

\subsubsection{Beyond CNF Classifiers.}
In the presented SSAT approach for verifying fairness, we assume the classifiers \red{$ \hat{y} $ (or $ \mathcal{A} $, why not $ \hat{Y} $)} to be represented as a CNF formula.  In the literature of interpretable machine learning, several studies have been conducted for learning CNF classifiers in the supervised learning setting, which include but are not limited to the work of~\cite{angelino2017learning,malioutov2018mlic,ghosh19incremental,yu2020computing}.  However, {\framework} can be extended beyond CNF classifiers, in particular to decision trees and linear classifiers that are widely adopted in the ML fairness studies~\cite{zemel2013learning,zafar2017fairness,xu2019achieving,zhang2019faht,raff2018fair,friedler2019comparative}.

\textbf{Decision Trees as CNF.} Existing  rule-based classifiers, for example, binary decision trees  can be trivially encoded as  CNF formulas.  In the binary decision tree, each node in the tree is a literal. A path  from the root to the leaf is a conjunction of literals (hence, a path is a clause) and the tree itself is a disjunction of all paths (or  clauses). In order to derive a CNF representation $ \phi $ of the decision tree, we first construct a DNF by considering all paths terminating at leaves with negative class label ($ \hat{y} = 0 $) and then complement it to a CNF using De Morgan's rule. Therefore, for any input that is classified positive by the decision tree satisfies $ \phi $ and vice versa. In {\frameworklearn} for learning the least favored group, we can construct a negated CNF classifier in Eq.~\ref{eq:er_complement} by only including paths terminating on positive labelled leaves. 

\textbf{Linear Classifiers as CNF.} Linear classifiers on Boolean attributes can  be encoded into CNF formulas using pseudo-Boolean encoding~\cite{philipp2015pblib}. We consider a linear classifier  $ W\cdot X + b \ge 0 $ on Boolean attributes $ X $ with weights $ W \in \mathbb{R}^{|X}| $ and bias $ b \in \mathbb{R} $.  We first normalize $ W$ and $b $ in $ [-1,1] $ and then round to integers so that the decision boundary becomes a pseudo-Boolean constraint, e.g., \textit{at-least} $ k $ constraint.  We then apply  pseudo-Boolean constraints to CNF translation to encode the decision boundary to CNF. This encoding usually introduces additional Boolean variables and results in large CNF. In order to generate a smaller CNF, we can apply thresholding techniques on the weights $ W $ to consider attributes with higher weights only. For instance, if the weight $  |w_i| \le \lambda $ for a threshold $ \lambda $ and $ w_i \in W $, we can set $ w_i = 0 $. Thus,  lower weighted (hence less important) attributes do not appear in the encoded CNF.  Finally, to construct the negated classifier in the SSAT formula in Eq.~\ref{eq:er_complement}, we encode $ W\cdot X + b < 0 $ to CNF using \textit{at-most} $ k $ encoding. 

In practical problems, attributes are generally real-valued or categorical. We next discuss how {\framework} can work beyond Boolean attributes. 


\subsubsection{Beyond Boolean Attributes.}
Classifiers that are already represented in CNF are usually trained on a Boolean abstraction of the input attributes where  each categorical attribute is one-hot encoded  and each real-valued attribute is discretized into a set of Boolean attributes~\cite{LKCL2019,GMM20}. Thus, {\framework} can verify CNF classifiers readily. 

\textbf{Decision Trees.} In case of binary decision tree classifiers, the input attributes are numerical or categorical, but each attribute is compared against a constant in each internal node of the tree. Hence, we fix a Boolean variable for each internal node where the Boolean assignment to the variable decides one of the two branches to choose from the current node.  

\textbf{Linear Classifiers.} Linear classifiers are generally trained on numerical attributes where we apply following discretization. Consider a numerical attribute $ x $ where $ w $ is its weight. We want to discretize $ x $ to a set $ \mathbf{B} $ of Boolean attributes and recalculate the weights of the variables in $ \mathbf{B} $ from $ w $. For discretization, we simply consider interval-based approach where for each interval (or bin) in the continuous space of $ x $, we consider a Boolean variable $ b_i \in \mathbf{B} $ such that $ b_i $ is assigned $ \top $ (or $ 1 $) when the attribute-value of $ x $ lies within the interval and $ b_i $ is assigned $ \bot $ (or $ 0 $) otherwise. Let $ \mu_i $ be the mean of the interval where  $ b_i $ can be $ \top $. We then fix the revised weight of $ b_i $ to be $ \mu_i\cdot w $. We can show trivially that if we consider infinite number of intervals, $ x \approx \sum_i \mu_ib_i $. 



