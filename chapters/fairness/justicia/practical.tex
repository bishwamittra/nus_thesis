\subsection{Practical Settings}
\label{sec:practical-setting}
We now relax the assumptions of {\justicia} on an access to Boolean classifiers and Boolean features, and extend {\justicia} to verify fairness metrics for more practical settings of decision trees, linear classifiers, and continuous features.

\subsubsection{Extension to Decision Trees and Linear Classifiers}
In the SSAT approach, we assume that the classifier $\alg$ is represented as a CNF formula.  
We extend {\justicia} beyond CNF classifiers to decision trees and linear classifiers, which are widely used in the fairness studies~\cite{zemel2013learning,raff2018fair,zhang2019faht}.
%{\color{red}In the literature of interpretable machine learning, several studies have been conducted for learning CNF classifiers in the supervised learning setting, which include but are not limited to the work of~\cite{angelino2017learning,malioutov2018mlic,ghosh19incremental}.  We leverage these techniques. (not needed)}

\textit{Binary decision trees} are trivially encoded as  CNF formulas.  In the binary decision tree, each node in the tree is considered as a literal. A \textit{path from the root to the leaf} is a conjunction of literals and thus, a \textit{clause}. The \textit{tree} itself is a disjunction of all paths and thus, a \textit{DNF (Disjunctive Normal Form)}. In order to derive a CNF of a decision tree, we first construct a DNF by including all paths terminating at leaves with negative class label ($ \widehat{Y} = 0 $) and then complement the DNF to CNF using De Morgan's rule. 

\textit{Linear classifiers on Boolean features} are encoded into CNF formulas using pseudo-Boolean encoding~\cite{philipp2015pblib}. We consider a linear classifier  $ \mathbf{W} \mathbf{X} + b \ge 0 $ on Boolean features $ \mathbf{X} $ with weights $ \mathbf{W} \in \mathbb{R}^{|\mathbf{X}|} $ and bias $ b \in \mathbb{R} $.  We first normalize $ \mathbf{W} $ and $ b $ in $ [-1,1] $ and then round to integers so that the decision boundary becomes a pseudo-Boolean constraint~\cite{roussel2009pseudo}.  Then we apply  pseudo-Boolean constraints to CNF translation~\cite{philipp2015pblib} to encode the decision boundary to CNF. This encoding usually introduces additional Boolean variables and results in a large CNF. In order to generate a smaller CNF, we can trivially apply thresholding  on the weights to consider features with higher weights only. For instance, if the weight $  |W_i| \le \lambda $ for a threshold $ \lambda  \in \mathbb{R}^+$ and $ W_i \in \mathbf{W} $, we can set $ W_i = 0 $. Thus, features with lower weights (less important) do not appear in the encoded CNF.  Moreover, all introduced variables in this CNF translation are given existential ($ \exists $) quantification and they appear in the inner-most position in the prefix of the SSAT formula. Thus, the presented ER-SSAT formulas become effectively ERE-SSAT formulas.

\subsubsection{Extension to Continuous Features}
In practical problems, features are generally real-valued or categorical but classifiers, which are naturally expressed as CNF (Chapter~\ref{chapter:imli}), are generally trained on a Boolean abstraction of input features. In order to perform the Boolean abstraction, each categorical feature is one-hot encoded and each real-valued feature is discretized into a set of Boolean features (Chapter~\ref{chapter:imli}). 

For a binary decision tree, each feature, including the continuous ones, is compared against a constant at each node (except leaves) of the tree. We assign a Boolean variable to each internal node of the tree (except leaves), where the $ \{0,1\} $ assignment to the variable decides one of the two branches to choose from the current node.  

Linear classifiers are generally trained on continuous features, where we apply discretization in the following way. Let us consider a continuous feature $X_c$, where $W$ is its weight during training. We discretize $ X_c $ to a set $ \mathbf{B} $ of Boolean features and recalculate the weight of each variable in $ \mathbf{B} $ based on $ W $. We consider the an interval-based approach for discretizing $ X_c $. For each interval in the continuous space of $X_c$, we consider a Boolean variable $B_i \in \mathbf{B}$, such that $ B_i $ is assigned $ 1 $ when the feature-value of $X_c$ lies within the $i^{\mathrm{th}}$ interval and $ B_i $ is assigned $ 0 $ otherwise. Following that, we assign the weight of $ B_i $ to be $ \mu_iW $, where $ \mu_i $ is the mean of feature values in the $i^{\mathrm{th}}$ interval. We can show that if we consider infinite number of intervals, $ X_c \approx \sum_i \mu_i B_i $. 



