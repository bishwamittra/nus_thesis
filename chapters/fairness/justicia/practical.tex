\subsection{Practical Settings}
\label{sec:practical-setting}
We now relax the assumptions on access to Boolean classifiers and Boolean attributes, and extend {\justicia} to verify fairness metrics for more practical settings of decision trees, linear classifiers, and continuous attributes.

\subsubsection{Extending to Decision Trees and Linear Classifiers. }
In the SSAT approach, we assume that the classifier $\alg$ is represented as a CNF formula.  
We extend {\justicia} beyond CNF classifiers to decision trees and linear classifiers, which are widely used in the fairness studies~\cite{zemel2013learning,raff2018fair,zhang2019faht}.
%{\color{red}In the literature of interpretable machine learning, several studies have been conducted for learning CNF classifiers in the supervised learning setting, which include but are not limited to the work of~\cite{angelino2017learning,malioutov2018mlic,ghosh19incremental}.  We leverage these techniques. (not needed)}

\textit{Binary decision trees} are trivially encoded as  CNF formulas.  In the binary decision tree, each node in the tree is a literal. A \textit{path from the root to the leaf} is a conjunction of literals and thus, a \textit{clause}. The \textit{tree} itself is a disjunction of all paths and thus, a \textit{DNF (Disjunctive Normal Form)}. In order to derive a CNF of a decision tree, we first construct a DNF by including all paths terminating at leaves with negative class label ($ \hat{Y} = 0 $) and then complement the DNF to CNF using De Morgan's rule. 

\textit{Linear classifiers on Boolean attributes} are encoded into CNF formulas using pseudo-Boolean encoding~\cite{philipp2015pblib}. We consider a linear classifier  $ W^T X + b \ge 0 $ on Boolean attributes $ X $ with weights $ W \in \mathbb{R}^{|X|} $ and bias $ b \in \mathbb{R} $.  We first normalize $W$ and $b $ in $ [-1,1] $ and then round to integers so that the decision boundary becomes a pseudo-Boolean constraint.  We then apply  pseudo-Boolean constraints to CNF translation to encode the decision boundary to CNF. This encoding usually introduces additional Boolean variables and results in large CNF. In order to generate a smaller CNF, we can trivially apply thresholding  on the weights to consider attributes with higher weights only. For instance, if the weight $  |w_i| \le \lambda $ for a threshold $ \lambda  \in \mathbb{R}^+$ and $ w_i \in W $, we can set $ w_i = 0 $. Thus, the attributes with lower weights and thus, less importance do not appear in the encoded CNF.  Moreover, all introduced variables in this CNF translation are given existential ($ \exists $) quantification and they appear in the inner-most position in the prefix of the SSAT formula. Thus, the presented ER-SSAT formulas become effectively ERE-SSAT formulas.

\subsubsection{Extending to Continuous Attributes.}
In practical problems, attributes are generally real-valued or categorical but classifiers, which are naturally expressed as CNF such as~\cite{GMM20}, are generally trained on a Boolean abstraction of the input attributes.
In order to perform this Boolean abstraction, each categorical attribute is one-hot encoded and each real-valued attribute is discretised into a set of Boolean attributes~\cite{LKCL2019,GMM20}. 

For a binary decision tree, each attribute, including the continuous ones, is compared against a constant at each internal node of the tree. We fix a Boolean variable for each internal node, where the Boolean assignment to the variable decides one of the two branches to choose from the current node.  

Linear classifiers are generally trained on continuous attributes, where we apply the following discretization. 
Let us consider a continuous attribute $X_c$, where $w$ is its weight during training. 
We discretize $ X_c $ to a set $ \mathbf{B} $ of Boolean attributes and recalculate the weight of each variable in $ \mathbf{B} $ based on $ w $. 
For the discretization of $X_c$, we consider the interval-based approach\footnote{Our implementation is agnostic to any discretization technique.}. 
For each interval in the continuous space of $X_c$, we consider a Boolean variable $B_i \in \mathbf{B}$, such that $ B_i $ is assigned TRUE when the attribute-value of $X_c$ lies within the $i^{\mathrm{th}}$ interval and $ B_i $ is assigned FALSE otherwise. 
Following that, we assign the weight of $ B_i $ to be $ \mu_i\times w $, when $ \mu_i $ is the mean of the $i^{\mathrm{th}}$ interval and  $ B_i $ is TRUE. 
We can show that if we consider infinite number of intervals, $ X_c \approx \sum_i \mu_i B_i $. 

