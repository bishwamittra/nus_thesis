\chapter{Fairness Verification with Feature Correlation}
	In this chapter, we discuss the extended experimental results as a continuation of Chapter~\ref{chapter:fvgm}.  


	\section{Extended Experimental Results}
	\label{fairness_fvgm_appendix:experiments}
	Each experiment is performed on Intel Xeon E$ 7-8857 $ v$2 $ CPUs with $ 16 $GB memory, $ 64 $bit Linux distribution based on Debian OS and clock speed $ 3 $ GHz.
	
	
	\input{tables/fairness/fvgm/sanity_check.tex}
		

	\subsection{Accuracy Comparison Among Different Verifiers}
	We have considered a synthetic problem for comparing accuracy among different verifiers. For Example~\ref{fvgm_example:intro}, we consider `age $ \ge 40 $' as a Bernoulli random  variable with probability $ 0.5 $. For `income' feature ($ I $), we consider two Gaussian distributions $ \Pr[I | \text{age} \ge 40] \sim \mathcal{N}(0.6, 0.1) $ and $ \Pr[I | \text{age} < 40] \sim \mathcal{N}(0.4, 0.1) $ separated by two age groups. Moreover, for `fitness' feature ($ F $), we consider two Gaussian distributions $ \Pr[F | \text{age} \ge 40] \sim \mathcal{N}(0.7, 0.1) $ and $ \Pr[F | \text{age} < 40] \sim \mathcal{N}(0.3, 0.1) $. On this data, the trained LR and SVM classifier has decision boundary as $ 7.26I + 7.4F - 1.34A \ge 6.62 $ and $ 9.37I + 9.75F - 0.34A \ge 9.4 $, respectively.
	
	
	In Figure~\ref{fairness_fvgm_fig:synthetic_bn} we show the Bayesian Network on discretized features, in particular for income and fitness features. In Figure~\ref{fairness_fvgm_fig:synthetic_ppv_max_LR} and Figure~\ref{fairness_fvgm_fig:synthetic_ppv_min_LR}, we show PPVs of logistic regression classifier computed by different verifiers, where  {\fvgm} outputs closest to exactly computed values, in comparison with Justicia. In Figure~\ref{fairness_fvgm_fig:synthetic_sample_size_LR_DI} and Figure~\ref{fairness_fvgm_fig:synthetic_sample_size_LR_SPD}, we show the effect of sample size on {\fvgm} in measuring fairness metrics: disparate impact and statistical parity, where with increasing sample size, the estimate becomes more accurate. Similar observations are recorded for SVM classifier in Figure~\ref{fairness_fvgm_fig:synthetic_results_SVM}.
	

		\begin{figure}
		\begin{center}
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Adult_LR}}
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Adult_SVM}}	\\		
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Compas_LR}}
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Compas_SVM}}\\
			
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_German_LR}}
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_German_SVM}}		\\
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Titanic_LR}}
			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Titanic_SVM}}\\
			
%			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Ricci_LR}}
%			\subfloat{\includegraphics[scale=0.4]{figures/fairness/fvgm/time_vary_features_Ricci_SVM}}
%			
			\caption{Effect of number of features on the runtime of different datasets for LR and SVM classifiers.}
			\label{fairness_fvgm_fig:time_vary_features}
			
			
			
		\end{center}
		
		
	\end{figure}
	
	
	\subsection{Scalability Comparison Among Different Verifiers}
	
	In Figure~\ref{fairness_fvgm_fig:time_vary_features}, we present the runtime of different fairness verifiers while varying the number of features in different datasets. We observe that with an increase of features, the runtime increases in general.
	
	
	
	
	
	
	
	
	\input{tables/fairness/fvgm/fairness_attacks.tex}
	%\input{tables/fairness/fvgm/verifier_scalability.tex}
	%\input{tables/fairness/fvgm/ssat_vs_subset_sum.tex}
	%\input{tables/fairness/fvgm/fairness_algorithms.tex}
	\input{tables/fairness/fvgm/fairness_metrics_on_compound_sensitive_groups.tex}	
	\input{tables/fairness/fvgm/dag_structure_complexity.tex}
		
	\subsection{Verifying Fairness Algorithms on Multiple Fairness Metrics}
	
	We show extended results on verifying fairness attack in Figure~\ref{fairness_fvgm_fig:attack_extended} for two fairness metrics: disparate impact (DI) and statistical parity (SP). We observe that {\fvgm} can detect poisoning attack for both metrics. 	In Figure~\ref{fairness_fvgm_fig:multiple_metrics} we show verification results on compound sensitive groups with respect to multiple fairness metrics. In this figure, we observe that with an increase in the number of groups, fairness metrics worsens\textemdash disparate impact decreases and other three metrics increases.
		
	\subsection{Performance Analysis of Bayesian Network}
	In Figure~\ref{fairness_fvgm_fig:results_DAG_complexity}, we analyze the performance of encoding Bayesian Networks of differing complexity. We define the complexity of the network as  $ \frac{|V|}{|\nonsensitive \cup \sensitive|} $, which is the ratio between the number of features appearing in the network and total features. In this figure, as the ratio increases, both computation time of {\fvgm} and learning time of Bayesian Network increase. 
	
	
	\begin{comment}
	\begin{table}
	
	\centering
	\small
	\caption{Extended results for verification of different fairness enhancing algorithms using {\fvgm}. Numbers in bold refer to fairness improvement w.r.t. different fairness metrics. RW and OP refer to reweighing and optimized-preprocessing algorithms respectively.  }
	\label{fairness_fvgm_fig:extended_verifying_fairness_algorithms}
	\setlength{\tabcolsep}{.5em}
	
	\begin{tabular}{lllrrrrrrrrrrrrr}
		
		\toprule
		Dataset & Sensitive & Algo. & $ \Delta $DI &  $ \Delta $PCF & $ \Delta $SP & $ \Delta$ EO\\
		\midrule
		
		
		\multirow{4}{*}{Adult}&\multirow{2}{*}{race}&RW&$ \textbf{0.53} $&$ \textbf{-0.06} $&$ \textbf{-0.06} $&$ \textbf{-0.02} $\\
		&&OP&$ \textbf{0.57} $&$ \textbf{-0.07} $&$ \textbf{-0.07} $&$ \textbf{-0.02} $\\
		\cmidrule{2-7}
		&\multirow{2}{*}{sex}&RW&$ \textbf{0.96} $&$ \textbf{-0.16} $&$ \textbf{-0.15} $&$ \textbf{-0.08} $\\
		&&OP&$ \textbf{0.43} $&$ \textbf{-0.08} $&$ \textbf{-0.08} $&$ 0.03 $\\
		
		\midrule
		\multirow{4}{*}{COMPAS}&\multirow{2}{*}{race}&RW&$ \textbf{0.13} $&$ \textbf{-0.07} $&$ \textbf{-0.07} $&$ \textbf{-0.06} $\\
		&&OP&$ \textbf{0.15} $&$ \textbf{-0.08} $&$ \textbf{-0.08} $&$ \textbf{-0.05} $\\
		\cmidrule{2-7}
		&\multirow{2}{*}{sex}&RW&$ \textbf{0.1} $&$ \textbf{-0.04} $&$ \textbf{-0.04} $&$ 0.04 $\\
		&&OP&$ \textbf{0.09} $&$ \textbf{-0.04} $&$ \textbf{-0.04} $&$ \textbf{-0.03} $\\
		
		\midrule
		\multirow{4}{*}{German}&\multirow{2}{*}{age}&RW&$ \textbf{0.52} $&$ \textbf{-0.53} $&$ \textbf{-0.52} $&$ \textbf{-0.47} $\\
		&&OP&$ \textbf{0.53} $&$ \textbf{-0.53} $&$ \textbf{-0.53} $&$ \textbf{-0.51} $\\
		\cmidrule{2-7}
		&\multirow{2}{*}{sex}&RW&$ -0.06 $&$ 0.06 $&$ 0.06 $&$ 0.02 $\\
		&&OP&$ -0.12 $&$ 0.12 $&$ 0.12 $&$ 0.07 $\\
		
		
		\bottomrule
	\end{tabular}
\end{table}
	
	\end{comment}

	
\iffalse
\begin{table}[h!]
		\centering
		\caption{Extended results for runtime of different verifiers in seconds.  `\textemdash'~ refers to timeout ($ 900 $ seconds). Bold number refers to best scalability results. We additionally show results for decision tree classifiers (DT). {\fvgm} takes comparatively more time for decision tree classifiers because of encoding feature-correlations represented as a Bayesian network.}
			\label{fairness_fvgm_tab:scalability_extended}	
		\begin{tabular}{llrrrrrrrrrrrrrr}
			
			\toprule
			Dataset & Classifier & FairSquare & VeriFair & Justicia & {\fvgm} \\
			\midrule
			
			\multirow{3}{*}{Ricci} & LR & \textemdash &  $ 2.0 $  &  $ 2.2 $  &  $ \textbf{0.1} $  \\ 
			& SVM & \textemdash &  $ 1.8 $  &  $ 2.2 $  &  $ \textbf{0.1} $  \\ 
			& DT &  $ 4.6 $  &  $ 1.9 $  &  $ \textbf{0.1} $  &  $ \textbf{0.1} $  \\ 
			
			
			\midrule
			\multirow{3}{*}{Titanic} & LR & \textemdash &  $ 0.5 $  &  $ 0.3 $  &  $ \textbf{0.1} $  \\ 
			& SVM & \textemdash &  $ 0.4 $  &  $ 0.2 $  &  $ \textbf{0.1} $  \\ 
			& DT &  $ 432.9 $  &  $ 17.2 $  &  $ \textbf{0.3} $  &  $ 1.6 $  \\ 
			
			
			\midrule
			\multirow{3}{*}{Compas} & LR & \textemdash &  $ 12.2 $  & \textemdash &  $ \textbf{0.4} $  \\ 
			& SVM & \textemdash &  $ 11.6 $  & \textemdash &  $ \textbf{0.5} $  \\ 
			& DT & \textemdash &  $ 377.1 $  &  $ \textbf{0.3} $  &  $ 121.7 $  \\ 
			
			
			\midrule
			\multirow{3}{*}{Adult} & LR & \textemdash &  $ 7.3 $  & \textemdash &  $ \textbf{0.9} $  \\ 
			& SVM & \textemdash &  $ 21.7 $  &  $ \textbf{1.7} $  &  $ 1.8 $  \\ 
			& DT & \textemdash &  $ 57.9 $  &  $ 0.4 $  &  $ \textbf{0.3} $  \\ 
			
			
			\midrule
			\multirow{3}{*}{German} & LR & \textemdash &  $ 19.2 $  & \textemdash &  $ \textbf{1.4} $  \\ 
			& SVM & \textemdash &  $ 28.8 $  & \textemdash &  $ \textbf{1.4} $  \\ 
			& DT & \textemdash &  $ 78.5 $  &  $ \textbf{0.2} $  &  $ 2.3 $  \\ 
			
			
			\bottomrule
			
			
		\end{tabular}
		
	\end{table}	
\fi		

%\newpage
\input{chapters/fairness/fvgm/fairness_influence_functions.tex}


\iffalse
\clearpage
\paragraph{Limitations of existing SSAT solvers.}
Since there is no state of the art SSAT solver that can handle universal quantification, we cannot consider conditional probabilities of universal variables while encoding the Bayesian network. In {\fvgm}, universal quantification appears while learning the least favored group in the learning encoding. When we do not consider conditional probabilities on universal variables, earlier approach could solve this using an existential SSAT solver by first negating the CNF and then subtracting computed probability from $ 1 $ as a solution to the original universal SSAT problem. This approach does not work when conditional probabilities are involved. 

For example, consider a clause $ a \vee\mathbf{B}\vee c $ defined on three Boolean variables where $ a $ is existential and $ b,c $ are random. Without considering conditional probabilities on $ a $, it is assigned true trivially. However, with conditional probabilities $ a $ can be false depending on the probability. Similar argument holds when $ a $ is universal. Hence, when conditional probabilities are considered for universal variables, a dedicated SSAT solver is also required to solve them. 
\fi





%\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}	




\input{chapters/appendix.tex}





