\section{Background: Fairness and Global Sensitivity Analysis}\label{sec:preliminaries}%\vspace*{-.5em}
Before proceeding to the details of our contribution, we present the fundamentals of group fairness metrics as quantifiers of bias and global sensitivity analysis as a classical method of feature attribution.%\vspace*{-.5em}
\subsection{Fairness in Machine Learning: Fairness Metrics}
%We consider\footnote{{We represent sets/vectors by bold letters, and the corresponding distributions by calligraphic letters. We express random variables in uppercase, and an assignment of a random variable in lowercase.}} a dataset $ \mathbf{D} $ as a collection of $n$ triples  $\{(\mathbf{x}^{(i)}, \mathbf{a}^{(i)}, y^{(i)})\}_{i=1}^n$  generated from an underlying distribution $\mathcal{D}$. Each non-sensitive data point $\mathbf{x}^{(i)}$ consists of $\numnonsensitive$ features $[x^{(i)}_1, \dots, x^{(i)}_{\numnonsensitive}] $. Each sensitive data point $\mathbf{a}^{(i)}$ consists of $\numsensitive$ categorical features $[a^{(i)}_1, \dots, a^{(i)}_{\numsensitive}] $.  $y^{(i)} \in \{0,1\}$ is the binary class corresponding to $(\mathbf{x}^{(i)}, \mathbf{a}^{(i)})$.  We denote the random variables corresponding to $ (\mathbf{x}, \mathbf{a}, y)$ with $ (\nonsensitive, \sensitive, Y) $.   We represent a binary classifier trained on dataset $\mathbf{D}$ as $\alg: (\nonsensitive, \sensitive) \rightarrow \widehat{Y} $. $\widehat{Y} \in \{0,1\}$ is the class predicted for $ (\nonsensitive, \sensitive) $. Given the setup, we discuss two fairness metrics quantifying the bias in the prediction of a classifier~\cite{feldman2015certifying,hardt2016equality,verma2018fairness}. 

%\blue{We consider\footnote{{We represent sets/vectors by bold letters, and the corresponding distributions by calligraphic letters. We express random variables in uppercase, and an assignment of a random variable in lowercase.}} a dataset $ \mathbf{D} $ as a collection of $n$ samples $\{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n$  generated from an underlying distribution $\mathcal{D}$. The feature vector $\mathbf{x}^{(i)}$ consists of $\numnonsensitive$ features $[x^{(i)}_1, \dots, x^{(i)}_{\numnonsensitive}] $.  $ \mathbf{x}^{(i)} $ additionally contains sensitive information represented as a vector $ \mathbf{a}^{(i)} \subset \mathbf{x}^{(i)} $. Specifically, the sensitive feature vector $\mathbf{a}^{(i)}$ consists of $\numsensitive$ categorical features $[a^{(i)}_1, \dots, a^{(i)}_{\numsensitive}] $.  $y^{(i)} \in \{0,1\}$ is the binary class corresponding to $ \mathbf{x}^{(i)} $.  We denote the random variables corresponding to $ (\mathbf{x}, \mathbf{a}, y)$ with $ (\nonsensitive, \sensitive, Y) $.   We represent a binary classifier trained on dataset $\mathbf{D}$ as $\alg: \nonsensitive \rightarrow \widehat{Y} $. $\widehat{Y} \in \{0,1\}$ is the class predicted for $ \nonsensitive $. Given the setup, we discuss two fairness metrics quantifying the bias in the prediction of a classifier~\cite{feldman2015certifying,hardt2016equality,verma2018fairness}. }

We consider\footnote{{We represent sets/vectors by bold letters, and the corresponding distributions by calligraphic letters. We express random variables in uppercase, and an assignment of a random variable in lowercase.}} a dataset $ \mathbf{D} $ as a collection of $n$ data points  $\{(\mathbf{z}^{(i)}, y^{(i)})\}_{i=1}^n$  generated from an underlying distribution $\mathcal{D}$. The feature vector $\mathbf{z}^{(i)} \triangleq (\mathbf{x}^{(i)}, \mathbf{a}^{(i)}) $ is a concatenation of non-sensitive features $ \mathbf{x}^{(i)} $ and sensitive features $ \mathbf{a}^{(i)} $. Each non-sensitive data point $\mathbf{x}^{(i)}$ consists of $\numnonsensitive$ features $[x^{(i)}_1, \dots, x^{(i)}_{\numnonsensitive}] \in \mathbb{R}^{\numnonsensitive} $. Each sensitive data point $\mathbf{a}^{(i)}$ consists of $\numsensitive$ categorical features $[a^{(i)}_1, \dots, a^{(i)}_{\numsensitive}] \in \mathbb{N}^{\numsensitive} $. Thus, the cardinality of feature vector $ \mathbf{z}^{(i)} $ is denoted by $ m $, formally $ |\mathbf{z}^{(i)}| \triangleq \numfeatures = \numnonsensitive + \numsensitive $. The binary class corresponding to $(\mathbf{x}^{(i)}, \mathbf{a}^{(i)})$ is $y^{(i)} \in \{0,1\}$. We refer to $y^{(i)}$ as the true class. We denote the random variables corresponding to $ (\mathbf{z}, \mathbf{x}, \mathbf{a},  y) $ with $ (\feature, \nonsensitive, \sensitive, Y) $.  Henceforth, we represent a binary classifier trained on dataset $\mathbf{D}$ as $\alg: (\nonsensitive, \sensitive) \rightarrow \widehat{Y} \in \{0,1\} $, where $\widehat{Y} $ is the predicted class. Given the setup, we discuss a fairness metric $ f: (\alg, \mathbf{D}) \rightarrow \mathbb{R}^{\ge 0} $ quantifying the bias in the prediction of a classifier on a dataset~\cite{feldman2015certifying,hardt2016equality,verma2018fairness}.



	\textit{Statistical Parity} ($ \mathsf{SP} $): Statistical parity belongs to the \textit{independence} measuring group fairness metrics that measures whether the prediction $ \widehat{Y} $ is statistically independent of the sensitive features $ \sensitive $.  The statistical parity of  a classifier is measured as $ f_{\mathsf{SP}}(\alg, \mathbf{D}) \triangleq \max_{\mathbf{a}}\Pr[\widehat{Y} =1 \mid \mathbf{A} = \mathbf{a}] - \min_{\mathbf{a}}\Pr[\widehat{Y} =1 \mid \mathbf{A} = \mathbf{a}] $, which is the difference between the maximum and minimum conditional probability of positive prediction the classifier for different sensitive groups. Lower value of $ f(\alg, \mathbf{D}) $ indicates higher fairness demonstrated by the classifier $\mathcal{M}$ on $ \mathbf{D} $. \textit{Henceforth, we deploy statistical parity as the measure of bias of a classifier w.r.t\ a given dataset.}\footnote{For brevity, we define other  fairness metrics equalized odds and predictive parity, and corresponding FIF computation technique in Appendix~\ref{app:pp}.}
	

	
	
%	\item \textit{Predictive Parity} ($ \mathsf{PP} $)~\citep{verma2018fairness}: \textit{Sufficiency} measuring group fairness metrics such as predictive parity constrain that the ground class $ Y $ is independent of $ \sensitive $ given the prediction $ \widehat{Y} $. Formally, $ f_{\mathsf{PP}}(\alg, \mathbf{D})  \triangleq \max(\max_{\mathbf{a}}\Pr[Y =1 \mid \mathbf{A} = \mathbf{a}, \widehat{Y} = 0] - \min_{\mathbf{a}}\Pr[Y =1 \mid \mathbf{A} = \mathbf{a}, \widehat{Y} = 0], \max_{\mathbf{a}}\Pr[Y =1 \mid \mathbf{A} = \mathbf{a}, \widehat{Y} = 1] - \min_{\mathbf{a}}\Pr[Y =1 \mid \mathbf{A} = \mathbf{a}, \widehat{Y} = 1])$. % \textit{Conditional Use Accuracy Equality (CUAE):}
%	\item \blue{\textit{Path-specific Causal Fairness} (PCF): 
%		Let $ \mathbf{a}_{\max}  \triangleq \argmax_{ \mathbf{a}} \Pr[\widehat{Y} =1 \mid\mathbf{A}=  \mathbf{a}] $ be the sensitive group with maximum conditional probability of positive prediction of the classifier. We consider mediator features $ \mediator \subseteq \nonsensitive $ sampled from the conditional distribution $ {\mathcal{Z}_{\mid\mathbf{A} = \mathbf{a}_{\max}}} $. This emulates the fact that mediator variables have the same sensitive features $ \mathbf{a}_{\max} $.  To this end, the path-specific causal fairness of a classifier is $ f(\alg, \mathbf{D}) = \max_{\mathbf{a}}\Pr[\widehat{Y} = 1 \mid \sensitive =  \mathbf{a}, \mediator] - \min_{\mathbf{a}} \Pr[\widehat{Y} = 1 \mid \sensitive = \mathbf{a}, \mediator ] $.}\change{How to sample $ \feature $?}

%\red{We observe that to yield a good estimator of statistical parity, the central function to estimate is $ \mathds{1}[\widehat{Y} = 1 \mid \sensitive = \mathbf{a}] $. Hereafter, we refer to it as  CPP (Conditional Positive Prediction) function.}

%\red{The group fairness metrics depend on the difference between different conditional probabilities of classifier's prediction. For all of these metrics, lower value of $ f(\alg, \mathbf{D}) $ indicates higher fairness demonstrated by the classifier $\mathcal{M}$ on $ \mathbf{D} $. \textit{We deploy these fairness metrics as the measures of bias of a classifier w.r.t\ a given dataset.}}\footnote{For brevity, we define other  fairness metrics equalized odds and predictive parity, and corresponding FIF computation technique in Appendix~\ref{app:pp}.}%\info{Not necessary!} 

\subsection{Global Sensitivity Analysis (GSA): Variance Decomposition}
Global sensitivity analysis is an active field of research that studies how the global uncertainty in the output of a function can be attributed to the different sources of uncertainties in the input variables~\citep{saltelli2008global}.
Sensitivity analysis is an essential component for quality assurance and impact assessment of models in EU~\citep{eu}, USA~\citep{usepa}, and research communities~\citep{saltelli2020five}.
%\paragraph{Sensitivity Analysis:}
\emph{Variance-based sensitivity analysis} is a form of global sensitivity analysis, where variance is considered as the measure of uncertainty~\citep{sobol1990sensitivity,sobol2001global}. To illustrate, let us consider a real-valued function $  \function(\feature) $, where $ \feature $ is a vector of $ \numfeatures $ input variables $ \{Z_1, \dots, Z_\numfeatures\} $.
% and $ Y $ is a univariate output. 
%\blue{In sensitivity analysis, assumptions are made on the independence and uniform distribution of $ X_i $'s within the unit hypercube}, that is, $ X_i \in [0,1] $ for $ i = \{ 1, \dots, k\} $.\footnote{In our experiments, we have not considered this yet.} Let $ [\numnonsensitive] \triangleq \{1,2,\dots, \numnonsensitive\} $. Then, 
Now, we decompose $ \function(\feature) $ among the subsets of inputs, such that:
\begin{align}
	\function(\feature) &= \function_0 + \sum_{i=1}^{\numfeatures} \function_{\{i\}}(Z_i) +  \sum_{i < j}^{\numfeatures} \function_{\{i,j\}}(Z_i, Z_j)  + \cdots\notag + \function_{\{1, 2, \dots, \numfeatures\}} (Z_1, Z_2, \dots, Z_{\numfeatures})\\
	&= \function_0 +  \sum_{\mathbf{S} \subseteq [\numfeatures]} \function_{\mathbf{S}}(\feature_{\mathbf{S}})\label{eq:functional_decomposition_set_notation}
\end{align}
%	$S_> = \{{S_0}, \ldots, {S_{|s|}}\}$ i.e. a monotonically increasing ordering of $|S|$ features for a given starting feature $Z_{S_0}$ such that $S_0 <S_1 <\ldots <S_{|S|}$.
The standard condition of this decomposition is the orthogonality of each term in the right-hand side of Eq.~\eqref{eq:functional_decomposition_set_notation}~\citep{sobol1990sensitivity}. In this decomposition, $ \function_0 $ is a constant, $ \function_{\{i\}} $ is a function of $ Z_i $, $ \function_{\{i,j\}} $ is a function of $ Z_i $ and $ Z_j $, and so on. Adhering to the set-based notations, we denote by $ g_{\mathbf{S}} $ a function of a non-empty subset of variables $ \feature_{\mathbf{S}}  \triangleq \{Z_i \mid i \in \mathbf{S}\} \subseteq \feature $, where $ \mathbf{S} = \{S_i \mid 1 \le |S_i| \le \numfeatures\} \subseteq [\numfeatures] $ is a non-empty subset of indices with $ [\numfeatures] \triangleq \{1,2,\dots, \numfeatures\}  $.  Here, $|\mathbf{S}|=1$ quantifies an individual variable's effect while $|\mathbf{S}|>1$ quantifies the higher-order intersectional effect of variables.
\iffalse
\begin{align}
	\label{eq:orthogonality_in_decomposition}
	\int_{0}^{1} \function_{\mathbf{S}}(\mathbf{X}_{\mathbf{S}})dX_i = 0, \text{ for } i \in \mathbf{S} = \{S_1, S_2, \dots, S_{|\mathbf{S}|}\}
\end{align}\info{let's discuss this}

This orthogonality constraint leads to the definitions of each term expressed as integrals (equivalently, the conditional expected values) of $ Y = f(\mathbf{X}) $. 

\begin{align*}
	&\int f(\mathbf{X})\prod_l dX_l =  \mathsf{E}[Y] = \function_0\\
	&\int f(\mathbf{X})\prod_{l \ne i} dX_l =  \mathsf{E}[Y\midX_i] = \function_{\{i\}}(X_i) + \function_0\\
	&\int f(\mathbf{X})\prod_{l \ne i, j} dX_l = \mathsf{E}[Y\midX_i, X_j] = \function_{\{i,j\}}(X_i, X_j)  +  \function_{\{i\}}(X_i) +  \function_{\{j\}}(X_j) + \function_0 \\
	&\vdots
\end{align*}

Thus, $ \function_{\{i\}} $ is the effect of varying $ X_i $ alone and it is known as the \emph{main effect} of $ X_i $. Similarly, $ \function_{\{i,j\}} $ is the effect of varying both $ X_i $ and $ X_j $ simultaneously, additional to their effect of individual variations. $ \function_{\{i,j\}} $ is thus known as the \emph{second-order interaction} between $ X_i $ and $ X_j $. Higher order interactions have analogous interpretations. 

Now, we assume $ f $ to be square integrable. Then each term $ \function_{\mathbf{S}} $ in Eq.~\eqref{eq:functional_decomposition_set_notation} are also square integrable. Squaring Eq.~\eqref{eq:functional_decomposition_set_notation}, integrating over $  [0,1]^\numfeatures$, and applying orthogonality constraint in Eq.~\ref{eq:orthogonality_in_decomposition}, we get
\begin{align*}
	\int f^2(\mathbf{X})\prod_l dX_l - \function_0^2 = \sum_{\mathbf{S} \subseteq [\numfeatures], \mathbf{S}\ne \emptyset}  \function_{\mathbf{S}}^2(\mathbf{X}_{\mathbf{S}}) \prod_{S_i \in \mathbf{S}} d Z_{S_i}
\end{align*}

Here, the left hand side is equal to the variance of $ Y = f(\mathbf{X}) $, and the terms in the right hand side are variance terms decomposed with respect to  $ \mathbf{X}_{\mathbf{S}} \subseteq \mathbf{X} $. 
\fi
Considering $ \function $ as square integrable, we obtain the decomposition of the variance of $ \function(\feature) $ expressed as the sum of variances of $\function_{\mathbf{S}}$'s~\citep{sobol1990sensitivity}.
\begin{align}\label{eq:variance_decomposition_set_notation}
	\mathsf{Var}[g(\feature)] &= \sum_{i=1}^{\numfeatures}V_{\{i\}} +  \sum_{i<j}^{\numfeatures} V_{\{i,j\}}  + \dots +  V_{\{1, 2\dots, \numfeatures\}}= \sum_{\mathbf{S} \subseteq [\numnonsensitive]} V_{\mathbf{S}} 
\end{align}
where $ V_{\{i\}} $ is the variance of $ \function_{\{i\}} $, $ V_{\{i,j\}} $ is the variance of $ \function_{\{i,j\}} $ and so on. Formally,  
\begin{equation}
\label{eq:decomposed_variance}
V_{\mathbf{S}} \triangleq \mathsf{Var} _{\feature_{\mathbf{S}}}\left[\mathbb{E}_{{\textbf {Z}}\setminus\feature_{\mathbf{S}}}[g(\feature)\mid \feature_{\mathbf{S}}]\right]- \sum_{\mathbf{S}' \subset \mathbf{S}}{V} _{\mathbf{S}'}.
\end{equation} 
Here, $\mathbf{S}'$ denotes all the non-empty and ordered proper subsets of $\mathbf{S}$. Thus, $ V_{\mathbf{S}} $ is the variance w.r.t.\ $ \feature_{\mathbf{S}} $ by subtracting the variances of all non-empty proper subsets of $ \feature_{\mathbf{S}} $. As a result, Eq.~\eqref{eq:variance_decomposition_set_notation} demonstrates how the variance of $\function(\feature) $ can be decomposed into terms attributable to each input feature, as well as the intersectional effects among them. Conversely, together all terms sum to the total variance of the model output. 
\textit{In the next section, we reduce the problem of computing FIFs of subsets of features into a variance decomposition problem.}

\iffalse

Formally, $V_{\{i\}} = \mathsf{Var}_{\mathbf{X}_{\{i\}}}[\mathsf{E}_{\mathbf{X}_{\sim \{i\}}}[Y \mid X_i]]$ and $V_{\{i,j\}} = \mathsf{Var}_{\mathbf{X}_{\{i,j\}}}[\mathsf{E}_{\mathbf{X}_{\sim \{i,j\}}}[Y \mid X_i, X_j]] - V_{\{i\}} - V_{\{j\}}$
\begin{align*}
	& V_{\{i\}} = \mathsf{Var}_{\mathbf{X}_{\{i\}}}[\mathsf{E}_{\mathbf{X}_{\sim \{i\}}}[Y \mid X_i]]\\
	& V_{\{i,j\}} = \mathsf{Var}_{\mathbf{X}_{\{i,j\}}}[\mathsf{E}_{\mathbf{X}_{\sim \{i,j\}}}[Y \mid X_i, X_j]] - V_{\{i\}} - V_{\{j\}}%\\
	%& \vdots
\end{align*}
$ \mathbf{X}_{\sim \mathbf{S}} \triangleq \mathbf{X} \setminus \mathbf{X}_{\mathbf{S}} $ notation indicates the set of all variables except that are in  $ \mathbf{X}_{\mathbf{S}} $. Thus, in each definition of $ V_{\mathbf{S}} $, the expectation of $ Y $ is computed on variables in $ \mathbf{X}_{\sim \mathbf{S}} $ and then, the variance of expectations is computed on variables in $ \mathbf{X}_{\mathbf{S}} $. 


This variance decomposition shows how the variance of $\function(\mathbf{X}) $ can be decomposed into terms attributable to each input, as well as the interactive effects among them. Together all terms sum to the total variance of the model output. 
\textit{We reduce the problem of computing FIFs of subsets of features into a variance decomposition problem.}
%	In the rest of the manuscript, we refer $ V_i $ as the first order variance of $ X_i $, $ V_{ij} $ as the second order variance of $ X_i $ and $ X_j $, and so on.


\paragraph{Law of total variance:}

if $ X $ and $ Y $ are random variables in the same probability space, and the variance of $ Y $ is finite, then law of total variance states the following.

\begin{align*}
	\mathsf{Var}[Y] = \mathsf{E}_X[\mathsf{Var}_Y[Y\midX]] + \mathsf{Var}_X[\mathsf{E}_Y[Y\midX]]
\end{align*}


Law of total variance is useful when we can compute the conditional variance/expectation of $ Y $ given $ X $ instead of the variance of $ Y $. 
\fi


