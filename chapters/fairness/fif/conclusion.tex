%\vspace{-0.5em}
\section{Chapter Summary}
%\vspace{-1em}
We propose the Fairness Influence Function (FIF) to quantify the contribution of input features on the resulting bias of a classifier for a given dataset. Relying on an additive axiom, we express group fairness metrics computed for sensitive groups as the sum of FIFs of all the subsets of non-sensitive features. To compute FIFs, we first prove existing group fairness metrics as the scaled difference of the conditional variances in the predictions of the classifier and then apply variance decomposition based on global sensitivity analysis. Finally, we propose {\fairXplainer}, an algorithm for efficiently and accurately computing FIFs by deploying a local regression to learn a set-additive decomposition of the classifier. The experimental results show that {\fairXplainer} estimates bias with significantly higher accuracy than the local explanation based approach SHAP. Also, {\fairXplainer} computes both individual and intersectional FIFs unlike SHAP to yield a better understanding of the sources of bias.
In future,  we aim to develop algorithms that leverage FIFs to yield unbiased decisions.