%\vspace{-0.5em}
\section{Chapter Summary}
%\vspace{-1em}


In this chapter, we propose the Fairness Influence Function (FIF) to measure the effect of input features on the bias of classifiers on a given dataset. Our approach combines global sensitivity analysis and group-based fairness metrics in machine learning. Thereby, it is natural in our approach to formulate FIF of intersectional features, which together with individual FIFs explains bias with higher granularity. We theoretically analyze the properties of FIFs and provide an algorithm, {\fairXplainer}, for estimating FIFs using global variance decomposition and local regression. In experiments, {\fairXplainer} estimates individual and intersectional FIFs in real-world datasets and classifiers, approximates bias using FIFs with less estimation error than earlier methods, demonstrates a high correlation between FIFs and fairness interventions, and analyzes the impact of fairness enhancing and attack algorithms on FIFs. The results instantiate {\fairXplainer} as a global, granular, and more accurate explanation method to understand the sources of bias. Additionally, the resonance between the rules extracted by CORELS~\cite{rudin19stop} and the most influential features detected by {\fairXplainer} indicates that FIFs can be exploited in future to create an explainable proxy of a biased/unbiased classifier. Also, we aim to extend {\fairXplainer} to compute FIFs for complex data, such as image and text, and design algorithms leveraging FIFs to yield unbiased decisions.
%