\chapter{Conclusion And Future Work}
\label{chapter:conclusion} 
Over the past decade, machine learning has been applied to various safety-critical domains, and it's crucial for classifiers to be interpretable, fair, robust, and private to ensure trustworthy and responsible AI. In this thesis, we focus on the interpretability and fairness aspects of machine learning and aim to improve the scalability and accuracy of the underlying problems. We utilize formal methods to make the following contributions: (i) In interpretable machine learning, we design an incremental learning technique for interpretable rule-based classifiers of varied expressiveness. (ii) In fairness in machine learning, we develop a formal probabilistic fairness verification framework that can verify multiple fairness definitions of classifiers.  Additionally, we develop techniques to explain fairness metrics by identifying feature combinations responsible for the bias of the classifier.



To demonstrate the efficacy of our methods, we have constructed open-source tools and conducted experiments on real-world datasets in machine learning to evaluate interpretability and fairness. In the context of interpretable rule-based machine learning, we have developed an incremental learning framework, known as {\imli}, that scales classification to million-size datasets while maintaining competitive prediction accuracy and rule-size compared to existing rule-based classifiers. Additionally, in our pursuit of more expressive yet interpretable classifiers, we have introduced another learning framework, called {\crr}, for logical relaxed classification rules that are based on incremental learning. Experimental results show that {\crr} is capable of learning more concise and accurate rule-based classifiers.


Our work on fairness in machine learning introduces two novel probabilistic fairness verifiers, {\justicia} and {\fvgm}, which exhibit superior performance in accuracy and scalability compared to state-of-the-art verifiers. {\justicia} is a stochastic-SAT-based verifier that enables scalable verification of fairness for compound sensitive groups of Boolean classifiers, such as decision trees, which was previously infeasible with existing methods. On the other hand, {\fvgm} takes correlated features into account and is capable of verifying the fairness of linear classifiers with higher scalability and accuracy than previous verifiers. Additionally, we propose a global sensitivity analysis-based method, {\fairXplainer}, that explains group fairness metrics by computing fairness influences of individual and intersectional features. Notably, {\fairXplainer} approximates bias more accurately using fairness influence functions (FIFs) and demonstrates a higher correlation of FIFs with fairness intervention than the local explainability-based approach.


In future, our research on fair and interpretable machine learning will focus on several areas. One immediate research question is to design a fairness improvement algorithm for machine learning classifiers using the fairness verifiers, {\justicia} and {\fvgm}, and the bias explanation framework, {\fairXplainer}. We also aim to extend formal fairness verification to more complex classifiers, such as random forests and neural networks. In addition, our future plans include deploying {\fairXplainer} on image classifiers, language models, and computer vision tasks. In a separate line of work, we will explore the application of incremental solving, as demonstrated in the context of interpretable classification learning by {\imli} and {\crr}, to other optimization problems, even beyond machine learning. To conclude the long-term vision of this thesis is to enhance machine learning in interpretability, fairness, and beyond, with the support of formal methods and automated reasoning.


\begin{comment}
	\begin{itemize}
		\item Fairness repair
		\item Incremental solving
		\item Fairness and interpretability as a service to more complex models.
	\end{itemize}
\end{comment}
