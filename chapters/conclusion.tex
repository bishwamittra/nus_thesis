\chapter{Conclusion And Future Work} In the past decade, the advancement of machine learning has witnessed a host of  applications in different safety-critical domains. In such domains, the interpretability, fairness, robustness, and privacy etc.\ of classifiers are considered important in trustworthy and responsible AI (artificial intelligence). In this thesis, we focus on the interpretability and fairness aspects of machine learning, where we prioritize on improving the scalability and accuracy of the underlying problems. Relying on formal methods, our contributions are summarized as follows: (i) in interpretable machine learning, we design incremental learning technique for interpretable rule-based classifiers, (ii) in fairness in machine learning, we develop a formal fairness verification framework to verify multiple fairness definitions, algorithms, and classifiers; and we develop techniques to identify the sources of unfairness of classifiers by computing fairness influence functions.


To validate the efficiency of our methods, we develop open-source tools and conduct experiments on real-world interpretability and fairness datasets in machine learning. In interpretable machine learning, our framework {\imli} scales classification to million-size datasets while achieving competitive prediction accuracy and rule-size compared to existing rule-based classifiers. In our quest for learning more expressible interpretable classifiers, we develop another learning framework {\crr} based on incremental learning. In experiments, {\crr} learns more concise and accurate rule-based classifiers while demonstrating scalability owing to incremental learning. 


In fairness in machine learning, our probabilistic fairness verifier {\justicia} and {\fvgm} demonstrates a superior performance in accuracy and scalability than the state of the art verifiers. Being a stochastic-SAT-based verifier, {\justicia} verifies fairness of compound sensitive groups of Boolean classifiers such as decision trees with higher scalability. {\fvgm}, in addition, considers correlated features and verifies fairness of linear classifiers with better scalability and accuracy than existing verifiers. Finally, given a classifier and a dataset, our framework {\fairXplainer} computes fairness influences of each subset of non-sensitive features as their contribution to the incurred bias of the classifier. In experiments, {\fairXplainer} estimates bias with significantly higher accuracy than the existing local explanation approach, while enabling computation of intersectional influences as a mean of better understanding the sources fo bias.
