\chapter{Conclusion And Future Work}
\label{chapter:conclusion} 
Over the past decade, machine learning has been applied to various safety-critical domains, and it's crucial for classifiers to be interpretable, fair, robust, and private to ensure trustworthy and responsible AI. In this thesis, we focus on the interpretability and fairness aspects of machine learning and aim to improve the scalability and accuracy of the underlying problems. We utilize formal methods to make the following contributions: (i) In interpretable machine learning, we design an incremental learning technique for interpretable rule-based classifiers of varied expressiveness. (ii) In fairness in machine learning, we develop a formal probabilistic fairness verification framework that can verify multiple fairness definitions of classifiers.  Additionally, we develop techniques to interpret fairness metrics by identifying feature combinations responsible for the bias of the classifier.



To demonstrate the efficacy of our methods, we have constructed open-source tools and conducted experiments on real-world datasets in machine learning. In the context of interpretable rule-based machine learning, we have developed an incremental learning framework, known as {\imli}, that scales classification to million-size datasets while maintaining competitive prediction accuracy and rule-size compared to existing rule-based classifiers. Additionally, in our pursuit of more expressive yet interpretable classifiers, we have introduced another learning framework, called {\crr}, for logical relaxed classification rules that are based on incremental learning. Experimental results show that {\crr} is capable of learning more concise and accurate rule-based classifiers.


Our work on fairness in machine learning introduces two novel probabilistic fairness verifiers, {\justicia} and {\fvgm}, which exhibit superior performance in accuracy and scalability compared to state-of-the-art verifiers. {\justicia} is a stochastic-SAT-based verifier that enables scalable verification of fairness for compound sensitive groups of Boolean classifiers, such as decision trees, which was previously infeasible with existing methods. On the other hand, {\fvgm} takes correlated features into account and is capable of verifying the fairness of linear classifiers with higher scalability and accuracy than previous verifiers. Additionally, we discuss a global sensitivity analysis-based method, {\fairXplainer}, that interprets group fairness metrics by computing fairness influences of individual and intersectional features. Notably, {\fairXplainer} approximates bias more accurately using fairness influence functions (FIFs) and demonstrates a higher correlation of FIFs with fairness intervention than the local explainability-based approach.


In future, our research on fair and interpretable machine learning will focus on several areas, which we elaborate in the following.

\begin{itemize}
	\item 
	\textbf{Post-hoc Explainability.} Black-box classifiers are hard to understand and trust, especially in safety-critical domains. In this thesis, we have designed interpretable classifiers that are explainable by design. A natural application of interpretable models is to apply them to explain the working of black-box classifiers. For example, rule-based classifiers can be leveraged to provide explanations in the form of classification rules, such as abduction-based explanations~\cite{ignatiev2019abduction}, Anchor~\cite{ribeiro2018anchors}, and LORE~\cite{guidotti2018local}. As such, we can use interpretable classifiers {\imli} and {\crr} to generate post-hoc explanations for black-box classifiers.  In future, we plan to test how well {\imli} can synthesize explanations with large samples and how well {\crr} can generate more expressive explanations.
	
	
	\item \textbf{Efficiency through Incremental Solving.} We aim to apply incremental solving, which we used for interpretable classification learning with {\imli} and {\crr}, to other optimization problems, even beyond machine learning. This can help us scale up solutions without losing much accuracy. For example, we aim to apply incremental solving in online learning of interpretable models, such as decision trees, decision lists, and decision sets, where the classifier can be routinely updated with new samples. Furthermore, we aim to try different objective functions in the incremental solving, apart from the hamming distance-based heuristic objective, which  works well in the incremental learning of CNF classification rules. One idea is to learn different rule-based classifiers, possibly in parallel, on different mini-batches or on different subsets of features, and then construct a rule merging procedure to combine different classification rules into one.  
	
	
	\item \textbf{Fairness Auditing.} Any technology that is publicly used presently undergoes an audit mechanism, where we understand the impacts and limitations of using that technology, and why are they caused. Machine learning is becoming the pervasive technology of our time and the discourse on bias induced by machine learning systems is attracting attention, e.g. the demonstration of bias in popular generative machine learning and large language models~\cite{abid2021persistent,nadeem2020stereoset,vig2020investigating}. As a result, there has been significant research interest in auditing classifiers for bias, from standard supervised learning to deep neural networks, computer vision, large language models and so on. Thus, we aim to design a fairness auditing framework~\cite{ruf2021towards,yan2022active} with formal guarantees.  There are three key questions that we aim to investigate in fairness auditing.
	
	\begin{enumerate}
		\item \textbf{Which fairness metrics to choose?} Fairness in machine learning is bestowed with multiple notions of fairness. Our first line of investigation is to categorize different fairness metrics  and suggest the best metric based on application, data, and prevailing policy, similar to ``Fairness Compass''~\cite{ruf2021towards}. This would help stakeholders pick the right definition of fairness for their application.
		
		\item \textbf{How to quantify bias?} Accurate quantification of bias is an important step towards designing algorithms to mitigate bias. As discussed in the thesis, fairness verification allows us to formally quantify the bias of a classifier. To this end, we aim to extend formal fairness verification to broader classes of fairness metrics, classifiers, and data. 
		
		
		\textbf{Fairness Metrics.} We aim to extend fairness verification of beyond group fairness, such as individual fairness~\cite{john2020verifying}, causal fairness~\cite{pan2021explaining,zhang2018fairness}, and counterfactual fairness~\cite{wu2019counterfactual,chiappa2019path}. Each category of fairness metrics poses distinct challenges in formal verification. For example, verifying individual fairness is related to verifying robustness of a model. As such, statistical methods from robustness verification has been applied to individual fairness~\cite{john2020verifying}. Formal methods, such as SMT-based encoding is also proposed in this regard~\cite{biswas2022fairify}. In our research endeavor, we aim to leverage  SAT or quantified Boolean formula (QBF) based verification, which may improve the scalability of verification.
		
		
		\textbf{Broader Classifiers.} We aim to extend fairness verification to broader machine learning classifiers, such as random forests and deep neural networks. For random forests, we can leverage CNF-based translation of the ensemble of trees by converting each tree as a CNF and a cardinality constraint to implement the ranking function of the prediction of multiple trees. For a special case of binarized neural networks (BNNs)~\cite{hubara2016binarized}, we can leverage existing CNF encoding and deploy SSAT-based verifier. However, for general neural networks with continuous parameters, MILP-based encoding can possibly be explored~\cite{mistry2022milp}. In addition, for neural networks, other surrogate representation beyond CNF or MILP can also be studied in future. In all cases, fairness verification, particularly for group-based metrics, relies on an access to efficient counter of CNF/MILP encoding; thus a dedicated effort to design better counter is a challenging, yet important research direction to explore.
		
		
		\textbf{Complex data.} We have explored fairness verification of tabular data; an important research question is to formally verify the fairness of classifiers with complex data such as images~\cite{nuriel2021permuted} and languages~\cite{abid2021persistent,nadeem2020stereoset,vig2020investigating}. For such data, one way to adapt existing verification methods, such as {\justicia}, is to apply them on the learned feature representation by the neural network and propagate verification results back to the input layer with images or text. In future, we aim to explore this possibility.
	
		\item \textbf{How to explain bias?} We aim to design fair and interpretable algorithms for  machine learning. We are interested in how these two goals relate to each other. This is inline with GDPR's emphasis on making models transparent and trustworthy, which is deployed in public.	Towards bridging the gap between fairness and interpretability in predictive systems, we explore following research questions:
		
		\begin{enumerate}
			\item How fair are interpretable machine learning models? 
			\item How can we improve the fairness of interpretable models?
			\item How can we apply interpretability to enhance fairness?
			\item How can we jointly optimize a classifier for  fairness and interpretability?
		\end{enumerate}
		
		Our work on {\fairXplainer} is a stepping stone towards bridging the gap between fairness and interpretability. We plan to explore more directions in the future.
		
	\end{enumerate}
	
	
	
	
 	
	
\end{itemize}


To conclude, the long-term vision of this thesis is to enhance machine learning in interpretability, fairness, and beyond. To achieve this goal, we aim to integrate formal methods and automated reasoning with statistics and algorithm design.

 

\begin{comment}
	\begin{itemize}
		\item Fairness repair
		\item Incremental solving
		\item Fairness and interpretability as a service to more complex models.
	\end{itemize}
\end{comment}
