\chapter{Conclusion And Future Work}
\label{chapter:conclusion} 
In the past decade, the advancement of machine learning has witnessed a host of  applications in different safety-critical domains. In such domains, the interpretability, fairness, robustness, and privacy etc.\ of classifiers are considered important in trustworthy and responsible AI (artificial intelligence). In this thesis, we focus on the interpretability and fairness aspects of machine learning, where we prioritize on improving the scalability and accuracy of the underlying problems. Based on formal methods, the contributions of our thesis are summarized as follows. (i) In interpretable machine learning, we design incremental learning technique for interpretable rule-based classifiers. (ii) In fairness in machine learning, we develop a formal fairness verification framework to verify multiple fairness definitions, algorithms, and classifiers; and we develop techniques to identify the sources of unfairness of classifiers by computing fairness influence functions.


To validate the efficiency of our methods, we develop open-source tools and conduct experiments on real-world interpretability and fairness datasets in machine learning. In interpretable machine learning, our framework {\imli} scales classification to million-size datasets while achieving competitive prediction accuracy and rule-size compared to existing rule-based classifiers. In our quest for learning more expressible interpretable classifiers, we develop another learning framework {\crr} for logical relaxed classification rules based on incremental learning. In experiments, {\crr} learns more concise and accurate rule-based classifiers while demonstrating scalability owing to the incremental learning. 


In fairness in machine learning, our probabilistic fairness verifier {\justicia} and {\fvgm} demonstrates a superior performance in accuracy and scalability than the state of the art verifiers. Being a stochastic-SAT-based verifier, {\justicia} verifies fairness of compound sensitive groups of Boolean classifiers such as decision trees with higher scalability. {\fvgm}, in addition, considers correlated features and verifies the fairness of linear classifiers with better scalability and accuracy than existing verifiers. Finally, given a classifier and a dataset, our framework {\fairXplainer} computes fairness influences of each subset of non-sensitive features as their contribution to the incurred bias of the classifier. In experiments, {\fairXplainer} estimates bias with significantly higher accuracy than the existing local explanation approach, while enabling computation of intersectional influences as a mean of better understanding the sources fo bias.

As a future work, we continue our research on fair and interpretable machine learning. An immediate research question is to design a fairness improvement or repair algorithm of machine learning classifiers, which benefits from fairness verifier such as {\justicia} and bias identification framework such as {\fairXplainer}. Another research question is to extend fairness verification to more complex classifiers, for example, neural networks. In addition, in future, we plan to deploy {\fairXplainer} on image classifiers, language models, and computer vision tasks. In a separate line of work, we aim to apply incremental solving, as presented in the context of interpretable classification learning, on other optimization problems, even beyond machine learning. Therefore, the long term vision of this thesis is to improve machine learning in interpretability, fairness and beyond, with the assistance of formal methods and automated reasoning.


\begin{comment}
	\begin{itemize}
		\item Fairness repair
		\item Incremental solving
		\item Fairness and interpretability as a service to more complex models.
	\end{itemize}
\end{comment}
