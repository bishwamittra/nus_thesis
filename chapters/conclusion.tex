\chapter{Conclusion And Future Work}
\label{chapter:conclusion} 
Over the past decade, machine learning has been applied to various safety-critical domains, and it's crucial for classifiers to be interpretable, fair, robust, and private to ensure trustworthy and responsible AI. In this thesis, we focus on the interpretability and fairness aspects of machine learning and aim to improve the scalability and accuracy of the underlying problems. We utilize formal methods to make the following contributions: (i) In interpretable machine learning, we design an incremental learning technique for interpretable rule-based classifiers of varied expressiveness. (ii) In fairness in machine learning, we develop a formal probabilistic fairness verification framework that can verify multiple fairness definitions of classifiers.  Additionally, we develop techniques to interpret fairness metrics by identifying feature combinations responsible for the bias of the classifier.



To demonstrate the efficacy of our methods, we have constructed open-source tools and conducted experiments on real-world datasets in machine learning. In the context of interpretable rule-based machine learning, we have developed an incremental learning framework, known as {\imli}, that scales classification to million-size datasets while maintaining competitive prediction accuracy and rule-size compared to existing rule-based classifiers. Additionally, in our pursuit of more expressive yet interpretable classifiers, we have introduced another learning framework, called {\crr}, for logical relaxed classification rules that are based on incremental learning. Experimental results show that {\crr} is capable of learning more concise and accurate rule-based classifiers.


Our work on fairness in machine learning introduces two novel probabilistic fairness verifiers, {\justicia} and {\fvgm}, which exhibit superior performance in accuracy and scalability compared to state-of-the-art verifiers. {\justicia} is a stochastic-SAT-based verifier that enables scalable verification of fairness for compound sensitive groups of Boolean classifiers, such as decision trees, which was previously infeasible with existing methods. On the other hand, {\fvgm} takes correlated features into account and is capable of verifying the fairness of linear classifiers with higher scalability and accuracy than previous verifiers. Additionally, we propose a global sensitivity analysis-based method, {\fairXplainer}, that interprets group fairness metrics by computing fairness influences of individual and intersectional features. Notably, {\fairXplainer} approximates bias more accurately using fairness influence functions (FIFs) and demonstrates a higher correlation of FIFs with fairness intervention than the local explainability-based approach.


In future, our research on fair and interpretable machine learning will focus on several areas, which we elaborate in the following.

\begin{itemize}
	\item 
	\textbf{Post-hoc Explainability.} Black-box classifiers are hard to understand and trust, especially in safety-critical domains. In this thesis, we have designed interpretable classifiers that are explainable by design. We can use these classifiers, such as {\imli} and {\crr}, to generate post-hoc explanations for black-box classifiers. In this context, logic-based explanations, such as abduction-based explanations~\cite{ignatiev2019abduction}, Anchor~\cite{ribeiro2018anchors}, and LORE~\cite{guidotti2018local} have demonstrated to be effective for this purpose. Therefore, we plan to test how well {\imli} can synthesize explanations with large samples and how well {\crr} can generate more expressive explanations.
	
	
	\item \textbf{Fairness Auditing.} Machine learning has demonstrated significant success as a predictive system in automated decision-making. However, quantifying and mitigating bias in these systems is still challenging, e.g. bias in generative machine learning and large language models~\cite{abid2021persistent,nadeem2020stereoset,vig2020investigating}. As a result, there has been significant research interest in auditing classifiers for bias, from standard supervised learning to deep neural networks, computer vision, large language models and so on. Thus, we need a fairness auditing framework~\cite{ruf2021towards,yan2022active} with formal guarantees. One task of this framework is to categorize different definitions of fairness metrics in the literature. This would help stakeholders pick the right definition of fairness for their application~\cite{ruf2021towards}. We plan to extend formal fairness verification beyond group fairness, such as individual fairness~\cite{john2020verifying}, causal fairness~\cite{pan2021explaining,zhang2018fairness}, and counterfactual fairness~\cite{wu2019counterfactual,chiappa2019path}. We also plan to cover more classes of machine learning classifiers that are hard or impossible to encode as CNF, such as deep neural networks. Additionally, we aim to extend fairness verification beyond tabular data to more complex data such as images and texts.
	
	
	\item \textbf{Fairness and Interpretability: Bridging the Gap.} We aim to design fair and interpretable algorithms for  machine learning. We are interested in how these two goals relate to each other. We have some research questions for the future: (i) are interpretable models fair? (ii) how can we improve the fairness of interpretable models? (iii) how can we use interpretability to enhance fairness? (iv) how can we optimize a classifier for both fairness and interpretability? Our work on {\fairXplainer} is a stepping stone towards bridging the gap between fairness and interpretability. We plan to explore more directions in the future.
	
	\item \textbf{Efficiency through Incremental Solving.} We aim to apply incremental solving, which we used for interpretable classification learning with {\imli} and {\crr}, to other optimization problems, not only in machine learning. This can help us scale up without losing much accuracy. For example, we aim to apply incremental solving in online learning of interpretable models, such as decision trees, decision lists, and decision sets. To conclude, the long-term vision of this thesis is to enhance machine learning in interpretability, fairness, and beyond, with the help of formal methods and automated reasoning.
\end{itemize}

 

\begin{comment}
	\begin{itemize}
		\item Fairness repair
		\item Incremental solving
		\item Fairness and interpretability as a service to more complex models.
	\end{itemize}
\end{comment}
