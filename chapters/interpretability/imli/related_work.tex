\section{Related Works}
\label{interpretability_imli_sec:related}

The progress in designing interpretable rule-based classifiers finds its root in the development of decision trees \cite{bessiere2009minimising,quinlan1986induction,quinlan1987simplifying}, decision lists \cite{rivest1987learning}, classification rules \cite{cohen1995fast} etc.  In early works, the focus was to improve the efficiency and scalability of the model rather than designing models that are interpretable. For example,  decision rule approaches such as  C4.5 rules \cite{quinlan2014}, CN2~\cite{ClarkN1989}, RIPPER \cite{cohen1995fast}, and SLIPPER  \cite{CohenS1999} 
rely on heuristic-based branch pruning and ad-hoc local criteria e.g., maximizing information gain, coverage, etc.

Recently, several optimization frameworks have been proposed for interpretable classification, where both accuracy and rule-size are optimized during training. For example,~\cite{malioutov2013exact} proposed exact learning of rule-based classifiers based on Boolean compressed sensing using a linear programming formulation.~\cite{su2016learning} presented two-level Boolean rules, where the trade-off between classification accuracy and interpretability is studied. In their work, the Hamming loss is used to characterize prediction accuracy, and sparsity is used to characterize the interpretability of rules.~\cite{wang2015falling} proposed a Bayesian optimization framework for learning falling rule lists, which is an ordered list of if-then rules. Other similar approaches based on Bayesian analysis for learning classification rules are~\cite{letham2015interpretable,wang2017bayesian}. Building on custom discrete optimization techniques,~\cite{angelino2017learning} proposed an optimal learning technique for decision lists using a branch-and-bound algorithm. In a separate study,~\cite{lakkaraju2016interpretable} highlighted the importance of decision sets over decision lists in terms of interpretability and considered a sub-modular optimization problem for learning a near-optimal solution for decision sets. Our proposed method for interpretable classification, however, relies on the improvement in formal methods over the decades, particularly the efficient CDCL-based solution for satisfiability (SAT) problems~\cite{silva2003grasp}. 



Formal methods, particularly SAT and its variants, have been deployed in interpretable classification in recent years. In the context of learning decision trees, SAT and MaxSAT-based solutions are proposed by~\cite{alos2021learning,janota2020sat,narodytska2018learning,shati2021sat}. In addition, researchers have applied SAT for learning explainable decision sets~\cite{ignatiev2018sat,ignatiev2021scalable,schidler2021sat,yu2020computing}. In most cases, SAT/MaxSAT solutions are not sufficient in solving large-scale classification tasks because of the $ \mathrm{NP} $-hardness of the underlying problem. This observation motivates us in combining MaxSAT with more practical algorithms such as incremental learning.  


Incremental learning has been studied in improving the scalability of learning problems, where data is processed in parts and results are combined to use lower computation overhead. In case of non-interpretable classifiers such as SVM, several solutions adopting incremental learning are available~\cite{syed1999incremental,ruping2001incremental}. For example,~\cite{cauwenberghs2001incremental} proposed an online recursive algorithm for SVM that learns one support-vector at a time. Based on radial basis kernel function,~\cite{ralaivola2001incremental} proposed a local incremental learning algorithm for SVM. In the context of deep neural networks, stochastic gradient descent is a well-known convex optimization technique\textemdash a variant of which includes computing the gradient on mini-batches~\cite{hinton2012neural,li2014efficient,masters2018revisiting}.  Federated learning, on the other hand, decentralizes training on multiple local nodes based on local data samples with only exchanging learned parameters to construct a global model in a central node~\cite{konevcny2015federated,konevcny2016federated}. Another notable technique is Lagrangian relaxation that decomposes the original problem into several sub-problems, assigns Lagrangian multipliers to make sure that sub-problems agree, and iterates by solving sub-problems and adjusting weights based on disagreements~\cite{fisher1981lagrangian,johnson2007lagrangian,lemarechal2001lagrangian}. To the best of our knowledge, our method is the first method that unifies incremental learning with MaxSAT based formulation to improve the scalability of learning rule-based classifiers.


Classifiers that are interpretable by design can be applied to improve the explainability of complex black-box machine learning classifiers. There is rich literature on extracting decompositional and pedagogical rules from non-linear classifiers such as support vector machines~\cite{barakat2004learning,barakat2005eclectic,diederich2008rule,martens2008rule,nunez2002rule} and neural networks~\cite{augasta2012rule,hailesilassie2016rule,setiono1995understanding,sato2001rule,zilke2016deepred,zhou2004rule}. In recent years, local model-agnostic approaches for explaining black-box classifiers  are  proposed by learning surrogate simpler classifiers such as rule-based classifiers~\cite{guidotti2018local,pastor2019explaining,rajapaksha2020lormika,ribeiro2018anchors}. The core idea in local approaches is to use a rule-learner that can classify synthetically generated neighboring samples with class labels provided by the black-box classifier. To this end, our framework {\imli} can be directly deployed as an efficient rule-learner and can explain the inner-working of black-box classifiers by generating interpretable rules.

