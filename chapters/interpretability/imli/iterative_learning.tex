\subsection{Iterative Learning}  We now discuss an iterative learning algorithm for rule-based classifiers.  The major advantage of iterative learning is that we solve a smaller MaxSAT query because of learning a \textit{partial} classifier in each iteration. Our iterative approach is motivated by the set-covering algorithm\textemdash also known as separate-and-conquer algorithm\textemdash in symbolic rule learning~\cite{furnkranz1999separate}. In this approach, the core idea is to define the \emph{coverage of a partial classifier} (for example, a clause in a CNF classifier). For a specific definition of coverage, this  algorithm separates samples covered by the partial classifier and recursively conquers remaining samples in the training data by learning another partial classifier until no sample remains. The final classifier is an aggregation of all partial classifiers\textemdash the conjunction of clauses in a CNF formula, for example. 

Iterative learning is different from mini-batch learning in several aspects. In mini-batch learning, we learn all clauses in a CNF formula together, while in iterative learning, we learn a single clause of a CNF in each iteration. Additionally, in mini-batch learning, we improve scalability by reducing the number of samples in the training data using mini-batches, while in iterative learning, we improve scalability by reducing the number of clauses to learn at once. Therefore, an efficient integration of iterative learning and mini-batch learning would benefit scalability from both worlds. In the following, we discuss this integration by first stating iterative learning for CNF classifiers.


In iterative learning, we learn one clause of a CNF classifier in each iteration, where the clause refers to a partial classifier. The coverage of a clause in a CNF formula is the set of samples that \emph{do not satisfy} the clause. The reason is that if a sample does not satisfy at least one clause in a CNF formula, the prediction of the sample by the full formula is class $ 0 $, because CNF is a conjunction of clauses. As a result, considering covered samples in the next iteration does not change their prediction regardless of whatever clause we learn in later iterations. To this end, a single clause learning can be performed efficiently by applying mini-batch learning discussed before. In Algorithm~\ref{interpretability_imli_algo:iterative_CNF_learning}, we provide an algorithm for learning a CNF classifier iteratively by leveraging mini-batch learning. This algorithm is a double-loop algorithm, where in the outer loop we apply iterative learning and in the inner loop, we apply mini-batch learning.


\begin{algorithm}
	\caption{Iterative CNF Classifier Learning}
	\label{interpretability_imli_algo:iterative_CNF_learning}
	\begin{algorithmic}[1]
		\Procedure{IterativeCNFLearning}{$ \mathbf{X},\mathbf{y},\lambda, k $}
		\State $ \mathcal{R} \leftarrow $ true \Comment{Initial formula}
		\For{$ i \leftarrow 1, \dots, k $ and $ \dataset \ne \emptyset $}
		\State $ C_i \leftarrow \textsc{MiniBatchLearning}( \dataset,\lambda, 1) $ \Comment{Single clause learning, $ k = 1 $}
		\label{interpretability_imli_algo:iterative_CNF_learning_incremental_learning}
		\State $ \dataset' \leftarrow \textsc{Coverage}(\dataset, C_i) $
		\label{interpretability_imli_algo:iterative_CNF_learning_coverage}
		\If{$ \dataset' = \emptyset $} \Comment{Terminating conditions}
		\State break
		\EndIf
		\State $ \mathcal{R} \leftarrow \mathcal{R}  \wedge C_i $
		\State $ \dataset  \leftarrow \dataset  \setminus \dataset' $ \Comment{Removing covered samples}
		\EndFor
		\State \Return $ \Rule $
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\paragraph{Terminating conditions.} In Algorithm~\ref{interpretability_imli_algo:iterative_CNF_learning}, we terminate iterative learning based on three conditions: (i) when $ \Rule $ contains all $ k $ clauses , (ii) the training data $ \dataset $ is empty (that is, no sample remains uncovered), and (iii) no new sample is covered by the current partial classifier. Since the first two conditions are trivial, we elaborate on the third condition. When clause $ C_i $ cannot cover any new sample from the training dataset $ \dataset $, the next iteration will result in the same clause $ C_i $ because the training data remains the same. In this case, we do not include clause $ C_i $ to classifier $ \Rule $ because of zero coverage. 




\section{Learning Other Interpretable Classifiers}
\label{interpretability_imli_sec:application}
In earlier sections, we discuss the learning of CNF classifiers using  {\imli}. {\imli} can also be applied to learning other interpretable rule-based representations. In this section, we discuss how {\imli} can be applied in learning DNF classifiers, decision lists, and decision sets.

\subsection{Learning DNF classifiers} 
\label{interpretability_imli_sec:dnf_learning}
For learning DNF classifiers, we leverage De Morgan's law where complementing a CNF formula results in a DNF formula. To learn a DNF classifier, say $ \Rule'(\nonsensitive) $, we can trivially show that $ Y = \Rule'(\nonsensitive) \leftrightarrow \neg (Y = \neg \Rule'(\nonsensitive)) $ for the feature vector $ \nonsensitive $. Here $ \neg \Rule'(\nonsensitive) $ is a CNF formula, by definition. Thus, we learn a DNF classifier by first complementing the class-label $ y^{(i)} $ to $ \neg y^{(i)} $ for each sample in the training dataset $ \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n $, learning a CNF classifier on $ \{(\mathbf{x}^{(i)}, \neg y^{(i)})\}_{i=1}^n $, and finally complementing the learned classifier to DNF. For example, the CNF classifier  ``(Male $ \vee $ Age $ < 50 $) $ \wedge $ (Education $ = $ Graduate $ \vee $ Income  $ \ge 1500 $)'' 	is complemented to a DNF classifier as 	 ``(not Male $ \wedge $ Age $ \ge 50 $) $ \vee $ (Education $ \ne $ Graduate $ \wedge $ Income $ \le 1500 $)''. 


To learn a DNF classifier incrementally, such as through mini-batch and iterative learning, we adopt the following procedure. For learning a DNF classifier using mini-batch learning, we first learn a CNF classifier on dataset $ \{(\mathbf{x}^{(i)}, \neg y^{(i)})\}_{i=1}^n $ and complement the classifier to a DNF classifier at the end of mini-batch learning. To learn a DNF classifier in the iterative approach, we  learn a single clause of the DNF classifier in each iteration, remove covered samples, and continue till no training sample remains. In this context, the coverage of a clause in a DNF formula is the set of samples satisfying the clause. 



\subsection{Learning Decision Lists}
In {\imli}, we apply an iterative learning approach for efficiently learning a  decision list. A decision list  $ \Rule_L $ is a list of pairs $ (C_1, V_1), \dots, (C_k, V_k) $, where we learn one pair in each iteration.  We note that the clause $ C_i $ is a conjunction of literals\textemdash equivalently, a single clause DNF formula. Hence, our task is to deploy {\imli} to efficiently learn a single clause DNF formula $ C_i $. In particular, we opt to learn this clause for the majority class, say $ V_i $, in the training dataset by setting the majority samples as class $ 1 $ and all other samples as class $ 0 $. As a result, even if the MaxSAT-based learning, presented in this chapter, is targeted for binary classification, we can learn a multi-class decision list in {\imli}. 


In Algorithm~\ref{interpretability_imli_algo:iterative_decision_lists_learning}, we present the iterative algorithm for learning decision lists.  In each iteration, the algorithm learns a pair $ (C_i, V_i) $, separates the training set based on the coverage of $ (C_i, V_i) $ and conquers the remaining samples recursively. The coverage of $ (C_i, V_i) $ is the set of samples that satisfies clause $ C_i $. Finally, we add a default rule $ (C_k, V_\mathsf{default}) $ to $ \Rule_L $ where $ C_k \triangleq  $ true denoting that the clause is satisfied by all samples. We select the default class $ V_\mathsf{default} $ in the following order: (i) if any class(s) is not in the predicted classes $ \{V_i\}_{i=1}^{k-1} $ of the decision list, 
 $ V_{\mathsf{default}} $ is the majority class among missing classes, and (ii)  $ V_{\mathsf{default}} $ is the majority class of the original training set $ \dataset $, otherwise. 


\begin{algorithm}
	\caption{Iterative Learning of Decision Lists}
	\label{interpretability_imli_algo:iterative_decision_lists_learning}
	\begin{algorithmic}[1]
		\Procedure{DecisionListLearning}{$ \dataset, \lambda, k $}
		\State $ \mathcal{R}_L \leftarrow \{\}$
		\For{$ i \leftarrow 1, \dots, k-1 $ and $ \dataset \ne \emptyset $}
		\State $ V_i \leftarrow \textsc{MajorityClass}(\{y^{(i)}\}_{i=1}^{|\dataset|}) $ \Comment{$ V_i $ specifies the target class given class label}
		\State $ C_i \leftarrow \textsc{MiniBatchDNFLearning}( \dataset, \lambda, 1, V_i) $ \Comment{Ref. Chapter~\ref{interpretability_imli_sec:dnf_learning}}
		
%		\label{interpretability_imli_algo:iterative_CNF_learning_incremental_learning}
		\State $ \dataset' \leftarrow \textsc{Coverage}(\dataset, C_i) $
%		\label{interpretability_imli_algo:iterative_CNF_learning_coverage}
		\If{$ \dataset' = \emptyset $} 
		\State break
		\EndIf
		\State $ \mathcal{R}_L \leftarrow \mathcal{R}_L \cup \{(C_i, V_i)\} $
		\State $ \dataset  \leftarrow \dataset  \setminus \dataset' $ 
		\EndFor
		\State $ \mathcal{R}_L \leftarrow \mathcal{R}_L  \cup \{(\text{true}, V_{\mathsf{default}} )\}$	\Comment{Default rule}
		\State \Return $ \Rule_L $
		\EndProcedure
	\end{algorithmic}
	
\end{algorithm}

\subsection{Learning Decision Sets}
We now describe an iterative procedure for learning decision sets. A decision set comprises of an individual clause-class  pair $ (C_i, V_i) $ where $ C_i $ denotes a single clause DNF formula similar to decision lists. In a decision set, a sample can satisfy multiple clauses simultaneously, which is attributed as an \textit{overlapping between clauses}~\cite{lakkaraju2016interpretable}. Concretely, the overlap between two clauses $ C_i $ and $ C_j $ with $ i \ne j $ is the set of samples $ \{\mathbf{x} | \mathbf{x} \models C_i \wedge \mathbf{x} \models C_j\} $ satisfying both clauses. One additional objective in learning a decision set is to minimize the overlap between clauses, as studied in~\cite{lakkaraju2016interpretable}. 
Therefore, along with optimizing accuracy and rule-sparsity, we discuss an iterative procedure for decision sets that additionally minimizes the overlap between clauses.

\begin{algorithm}
	\caption{Iterative Learning of Decision Sets}
	\label{interpretability_imli_algo:iterative_decision_sets_learning}
	\begin{algorithmic}[1]
		\Procedure{DecisionSetsLearning}{$\dataset,\lambda, k$}
		\State $ \mathcal{R}_S=\{\} $
		\State $ \dataset_{cc} = \{\}$  \Comment{Contains correctly covered samples}
		\For{$ i \leftarrow 1, \dots, k-1 $ and $ \dataset \ne \emptyset $}
		\State $ V_i \leftarrow \textsc{MajorityClass}(\{y^{(i)}\}_{i=1}^{|\dataset|}) $
		\State $ \dataset_{w} = \dataset  \cup \{(\mathbf{x}, \neg V_i) | (\mathbf{x}, y) \in \dataset_{cc}\} $ \Comment{Appending covered samples with complemented class }
%		\State $ \mathbf{X}^\text{w} \leftarrow \mathbf{X} \cup \mathbf{X}^\text{cc} $ \Comment{Correctly covered samples are included}
%		\State $ \mathbf{y}^\text{w} \leftarrow  \mathbf{y} \cup  \{\neg V_i\}^{|\mathbf{X}^\text{cc}|}$ \Comment{$ \mathbf{X}^\text{cc} $ have complemented class $ \neg V_i $ to minimize overlap}
		\State $ C_i \leftarrow \textsc{MiniBatchDNFLearning}( \dataset_w,\lambda, 1, V_i) $ 
		\State $ \dataset' \leftarrow \textsc{CorrectCoverage}(\dataset, C_i, V_i) $
		\If{$ (\dataset' = \emptyset $} 
		\State break
		\EndIf
		\State $ \mathcal{R}_S \leftarrow \mathcal{R}_S \cup \{(C_i, V_i)\} $
		\State $ \dataset \leftarrow \dataset  \setminus \dataset' $
		\State $ \dataset_{cc} = \dataset_{cc} \cup \dataset' $
%		\State $ \mathbf{X}^\text{cc} = \mathbf{X}^\text{cc} \cup \mathbf{X}'$ 
		\EndFor
		\State $ \mathcal{R}_S \leftarrow \mathcal{R}_S  \cup \{(\text{true}, V_{\mathsf{default}} )\}$
		\State \Return $ \Rule_S $
		\EndProcedure
	\end{algorithmic}
\end{algorithm} 






In Algorithm~\ref{interpretability_imli_algo:iterative_decision_sets_learning}, we present an iterative algorithm for learning a decision set. The iterative algorithm is a modification of separate-and-conquer algorithm  by additionally focusing on minimizing overlaps in a decision set.  Given a training data $ \dataset = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n $, a  regularization parameter $ \lambda $, and the number of clauses $ k $, the core idea of Algorithm~\ref{interpretability_imli_algo:iterative_decision_sets_learning} is to learn a pair $ (C_i,V_i) $ in each iteration, separate covered samples from $ \dataset $, and conquer remaining samples recursively. In contrast to learning decision lists, we have following modifications in Algorithm~\ref{interpretability_imli_algo:iterative_decision_sets_learning}. 

\begin{itemize}
	\item The first modification is with respect to the definition of coverage for decision sets. Unlike decision lists, we separate samples that are \textit{correctly covered} by $ (C_i, V_i) $ in each iteration. Given  a dataset $ \dataset = \{(\mathbf{x}^{(l)}, y^{(l)})\}$, the correctly covered samples of $ (C_i, V_i) $ is a dataset $ \dataset_{cc}  =  \{(\mathbf{x}^{(l)}, y^{(l)}) | \mathbf{x}^{(l)} \models C_i \wedge y^{(l)} = V_i \}_{l=1}^{|\dataset|} \subseteq \dataset $ of samples that satisfy $ C_i $ and have matching class-label as $ V_i $. 
	\item The second modification is related to the  training dataset considered in each iteration. Let $ \dataset $ denote \emph{the remaining training dataset} in the current iteration and $ V_i $ be the majority class in  $ \dataset $. Hence, $ V_i $ is the target class in the current iteration. Also, let $ \dataset_{cc} $ denotes the set of samples \emph{correctly covered in all previous iterations}. In Algorithm~\ref{interpretability_imli_algo:iterative_decision_sets_learning}, we learn the current rule $ C_i $ on the working dataset $ \dataset_{w} = \dataset  \cup \{(\mathbf{x}, \neg V_i) | (\mathbf{x}, y) \in \dataset_{cc}\} $. Thus, $ \dataset_{w} $ is constructed by joining the remaining training samples with correctly covered samples in $ \dataset_{cc} $ with complemented class label to the target class $ V_i $.  Thus, by explicitly labeling covered samples as class $ \neg V_i $, the new clause $ C_i $ learns to falsify already covered samples. This heuristic allows us to minimize the overlap of $ C_i $ compared to previously learned clauses $ \{C_j\}_{j=1}^{i-1} $.
\end{itemize}

Finally, the default clause for decision sets is learned similarly as in decision lists. 





\newcommand{\maxsatquery}{\ensuremath{\mathsf{ConstructQuery}}}
\newcommand{\createclause}{\ensuremath{\mathsf{ConstructClause}}}
\newcommand{\iterativeClauseLearning}{\ensuremath{\mathsf{IterativeClauseLearning}}}
\let\oldReturn\Return
\renewcommand{\Return}{\State\oldReturn}