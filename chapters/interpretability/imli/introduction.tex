\label{chapter:imli}
%Machine learning models are presently deployed in safety-critical and high-stake decision-making arising in medical~\cite{erickson2017machine,kaissis2020secure,kononenko2001machine}, law~\cite{kumar2018law,surden2014machine}, and transportation~\cite{peled2019model,zantalis2019review}. In safety-critical domains, a surge of interest has been observed in designing interpretable, robust, and fair models instead of merely focusing on an accuracy-centric learning objective~\cite{bhagoji2018enhancing,doshi2017towards,du2019techniques,hancox2020robustness,holstein2019improving,mehrabi2021survey,molnar2020interpretable,murdoch2019interpretable}. 
In this chapter, we learn an interpretable rule-based classifier~\cite{rudin2019stop}, where the rules governing the prediction are explicit (by design) in contrast to black-box classifiers. 
%In order to achieve the interpretability of predictions, decision functions in the form of classification rules such as decision trees,  decision lists, decision sets, etc.\ are particularly effective~\cite{bessiere2009minimising,dash2021lprules,ignatiev2021reasoning,izza2020explaining,lakkaraju2017interpretable,lakkaraju2016interpretable,letham2015interpretable,narodytska2018learning,rivest1987learning,wang2015falling,yu2020optimal}. Such models are used to either learn an interpretable model from the start or as proxies to provide post-hoc explanations of pre-trained black-box models~\cite{gill2020responsible,lundberg2017unified,moradi2021post,ribeiro2016should,slack2020fooling}.  In our context of rule-based classifiers, we use the sparsity of rules as a measure of interpretability, which is adopted in various domains, specifically in the medical domain~\cite{gage2001validation,lakkaraju2019faithful,letham2015interpretable,malioutov2013exact,myers1962myers}.  
In particular, we study classifiers expressed as Boolean formulas, wherein we define interpretability in terms of the number of Boolean literals that the formula contains. Here, we illustrate an interpretable classifier that decides if a tumor cell is malignant or benign based on different features of tumors.

\vspace{1em}
\boxed{
\begin{array}{rl}
	& \text{A tumor is malignant if}\\
	&[(\text{compactness SE} < 0.1) \text{\textbf{ OR }}  \neg (0.1 \le \text{concave points} < 0.2)]
	\text{\textbf{ \blue{AND} }} \\
	&[\neg (0.2 \le \text{area} < 0.3) \text{\textbf{ OR }} \neg (0.1 \le \text{largest symmetry }< 0.2)]
\end{array}
}
\vspace{1em}


\begin{example}
	We illustrate an interpretable classification rule in CNF for classifying a tumor cell. We consider WDBC (Wisconsin Diagnostic Breast Cancer) dataset~\cite{agarap2018breast} to learn a CNF formula with two \emph{clauses} and four Boolean \emph{literals}. In the formula, clauses are connected by Boolean `AND', where each clause contains \emph{literals} connected by Boolean `OR'. Informally, a tumor cell is malignant if at least one literal in each clause becomes true.
\end{example}



The problem of learning rule-based classifiers is known to be computationally intractable. The earliest tractable approaches for classifiers such as decision trees and decision lists relied on heuristically chosen objective functions and greedy algorithmic techniques~\cite{ClarkN1989,CohenS1999,quinlan2014}. In these approaches, the size of rules is controlled by early stopping,  ad-hoc rule pruning, etc. In recent approaches, the interpretable classification problem is reduced to an optimization problem, where the accuracy and the sparsity of rules are optimized jointly~\cite{lakkaraju2016interpretable,narodytska2018learning}. Different optimization solvers such as linear programming~\cite{malioutov2013exact}, sub-modular optimizations~\cite{lakkaraju2016interpretable}, and Bayesian optimizations~\cite{letham2015interpretable} are then deployed to find the best classifier with maximum accuracy and minimum rule-size. In our study, we propose an alternate optimization approach that fits particularly well to rule-learning problems. Particularly,  we propose a \textit{maximum satisfiability} (MaxSAT)  solution for learning interpretable rule-based classifiers. 

The MaxSAT problem is an optimization analog to the satisfiability (SAT) problem, which is complete for the class $ \mathrm{FP}^{\mathrm{NP}} $. Although the MaxSAT problem is NP-hard, dramatic progress has been made in designing solvers that can handle large-scale problems arising in practice. This has encouraged researchers to reduce several optimization problems into MaxSAT such as  optimal planning~\cite{robinson2010partial}, automotive configuration~\cite{walter2013applications}, group-testing~\cite{CGSM2020}, data analysis in machine learning~\cite{berg2019applications}, and automatic test pattern generation for cancer therapy~\cite{lin2012application}. 



\paragraph{Contributions.} The primary contribution of this chapter is a MaxSAT-based formulation, called {\imli} (\textbf{I}ncremental \textbf{M}axSAT-based \textbf{L}earning of \textbf{I}nterpretable classification rules), for learning interpretable classification rules expressible in propositional logic. For the simplicity of exposition, our initial focus is on learning classifiers expressible in CNF formulas; we later discuss how the CNF learning formulation can be extended to popular interpretable classifiers such as decision lists and decision sets. In this chapter, {\imli} provides precise control to jointly optimize the accuracy and the size of classification rules. {\imli} constructs and solves a MaxSAT query of $ \mathcal{O}(kn) $  size (number of clauses) to learn a $ k $-clause CNF formula on a dataset containing $ n $ samples. Naturally, the na\"ive formulation cannot scale to large values of $ n $ and $ k $. To scale {\imli} to large datasets, we propose an incremental learning technique along with the MaxSAT formulation. Our incremental learning is an integration of mini-batch learning and iterative rule-learning, which are studied separately in classical learning problems. In the presented incremental approach, {\imli} learns a $ k $-clause CNF formula using an iterative separate-and-conquer algorithm, where in each iteration, a single clause is learned by covering a part of the training data. Furthermore, to efficiently learn a single clause in a CNF, {\imli} relies on mini-batch learning, where it solves a sequence of smaller MaxSAT queries corresponding to each mini-batch. 



In our experimental evaluations, {\imli} demonstrates the best balance among prediction accuracy, interpretability, and scalability in learning classification rules. In particular, {\imli} achieves competitive prediction accuracy and interpretability w.r.t.\ state-of-the-art interpretable classifiers. Besides, {\imli} achieves impressive scalability by classifying datasets even with $ 1 $ million samples, wherein existing classifiers, including those of the non-interpretable ones, either fail to scale or achieve poor prediction accuracy. Finally, as an application, we deploy {\imli} in learning interpretable classifiers such as decision lists and decision sets.



%The presented framework {\imli} extends our prior chapters ~\cite{ghosh19imli,MM18} in the following aspects. \cite{MM18} presented a MaxSAT-based formulation for interpretable classification rule-learning, which jointly optimizes between the accuracy and size of rules.  \cite{ghosh19imli} improved the MaxSAT formulation by unifying mini-batch learning. This unification allows learning on moderate-size datasets (about $ 50 $ thousand samples). This chapter takes a significant step in terms of scalability by integrating both iterative learning and mini-batch learning inside the incremental technique. The result is a practical scalable learning framework that can classify datasets with $ 1 $ million samples and can potentially scale beyond that.   


%We organize the rest of the chapter as follows. We discuss related literature in Section~\ref{interpretability_imli_sec:related} and preliminaries in Section~\ref{interpretability_imli_sec:preliminaries}. In Section~\ref{interpretability_imli_sec:problem}, we formally define our learning problem and provide a MaxSAT-based solution in Section~\ref{interpretability_imli_sec:baseline}. We improve the MaxSAT-based formulation through an incremental learning technique in Section~\ref{interpretability_imli_sec:incremental_learning}. In Section~\ref{interpretability_imli_sec:application}, we apply {\imli} in learning different interpretable classifiers. We then discuss our experimental results in Section~\ref{interpretability_imli_sec:experiments} and conclude in Section~\ref{interpretability_imli_sec:conclusion}. 








