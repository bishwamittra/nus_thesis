
\section{Chapter Summary}
\label{interpretability_imli_sec:conclusion}
Interpretable machine learning is gaining more focus with applications in many safety-critical domains. Considering the growing demand for interpretable models, it is challenging to design learning frameworks that satisfy all aspects: being accurate, interpretable, and scalable in practical classification tasks.  In this chapter, we have proposed a MaxSAT-based framework {\imli} for learning interpretable rule-based classifiers expressible in CNF formulas. {\imli} is built on efficient integration of incremental learning, specifically mini-batch and iterative learning, with MaxSAT-based formulation.  In our empirical evaluation, {\imli} achieves the best balance among prediction accuracy, interpretability, and scalability. In particular, {\imli} demonstrates competitive prediction accuracy and rule-size compared to existing interpretable rule-based classifiers. In addition, {\imli} achieves impressive scalability than both interpretable and non-interpretable classifiers by learning interpretable rules on million-size datasets with higher accuracy.  Finally, {\imli}  generates other popular interpretable classifiers such as decision lists and decision sets using the same framework. An interesting direction is to leverage the incremental learning in {\imli} to learn more expressible yet interpretable classification rules, which we explore in the next chapter.


%\section*{Acknowledgments}
%This work was supported in part by National Research Foundation, Singapore under its NRF Fellowship Programme [NRF-NRFFAI1-2019-0004 ] and AI Singapore Programme [AISG-RP-2018-005],  and NUS ODPRT Grant [R-252-000-685-13]. The computational work for this article was performed on resources of Max Planck Institute for Software Systems, Germany and the National Supercomputing Centre, Singapore (\url{https://www.nscc.sg}.