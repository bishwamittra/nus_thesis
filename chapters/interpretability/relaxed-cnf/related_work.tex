\section{Related Work}
\label{interpretability_crr_sec:related_work}
In the growing field of interpretable machine learning,  several rule-based systems such as decision trees~\cite{narodytska2018learning},  decision lists~\cite{R1987}, decision sets~\cite{ignatiev2018sat,lakkaraju2016interpretable} and classification rules~\cite{C1995,DGW2018} have been proposed over the years. More recently, a Bayesian framework has been adopted to learn rule sets~\cite{WRDLKM2017}, rule lists \cite{LRMM2015},  and falling rule lists~\cite{WR2015}. Building on the connection between rule learning and Boolean compression sensing, Malioutov and Varshney proposed a rule-based classification framework to trade-off classification accuracy and representation size~\cite{MV2013}. Furthermore, Su et al.~\cite{SWVM2015} proposed an extension that allows learning two-level Boolean rules. In addition to designing classifiers that produce representations amenable to the end users, there is a large body of work to describe the prediction of opaque black-box models~\cite{LKCL2019,lundberg2017unified,ribeiro2016should}.  While our work can also be applied to interpret black-box classifiers, we do not pursue this direction in this paper and leave for future work. 

Recently,  Ghosh and Meel~\cite{ghosh19incremental} propose an incremental approach for efficiently learning CNF classification rules using a MaxSAT-based framework. They adopt a sequential partition-based training methodology in incremental learning to achieve scalability in learning. Although the said approach achieves an improvement in runtime,  the generated rules highly depend on the order of samples in the training dataset. In contrast, the incremental framework proposed in this work does not suffer from the strong dependencies on the order of samples in training.\change{Do we keep this related work?}

