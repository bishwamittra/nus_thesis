\section{Related Work}
\label{interpretability_crr_sec:related_work}
In the growing field of interpretable machine learning,  several rule-based systems such as decision trees~\cite{narodytska2018learning},  decision lists~\cite{R1987}, decision sets~\cite{ignatiev2018sat,lakkaraju2016interpretable} and classification rules~\cite{C1995,DGW2018} have been proposed over the years. More recently, a Bayesian framework has been adopted to learn rule sets~\cite{WRDLKM2017}, rule lists \cite{LRMM2015},  and falling rule lists~\cite{WR2015}. Building on the connection between rule learning and Boolean compression sensing,  a rule-based classification system was proposed to trade-off classification accuracy and representation size~\cite{MV2013}. Su et al.~\cite{SWVM2015} proposed an extension that allows two-level Boolean rules. In addition to designing classifiers that produce forms amenable to end-users, there is a large body of work to describe the working of opaque models (model-agnostic interpretability)~\cite{LKCL2019,lundberg2017unified,ribeiro2016should}.  While our work can also be applied to interpret black-box classifiers, we do not pursue this direction in this paper and leave for future work. 


In recent work,  Ghosh and Meel~\cite{ghosh19incremental} propose an incremental approach for efficiently learning CNF classification rules using a MaxSAT-based framework. They adopt a sequential partition-based training methodology in incremental learning. Although the said approach achieves the runtime improvement,  the generated rules highly depend on the order/position of samples in the training dataset. In contrast, the incremental framework proposed in this work does not suffer from strong dependencies on the order of samples in the training dataset.

