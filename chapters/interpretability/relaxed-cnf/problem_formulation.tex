%\vspace{-2pt}
\section{Problem Formulation}
\label{interpretability_crr_sec:problem}

Given 
\begin{enumerate}
	\item a dataset $ \dataset = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n$ of $ n $ samples, where feature vector $ \mathbf{x}^{(i)} \in \{0, 1\}^m $ contains $ m $ features and class label $ y^{(i)} \in \{0,1\} $,
	\item a positive integer $ k \ge 1 $ denoting the number of clauses to be learned in the classification rule, 
	\item an integer threshold on literals $ \card_{l} \in  \{0,\dots,m\} $ indicating  the  minimum number of literals required to be true  to satisfy a clause,
	\item an  integer threshold on clauses $ \card_{c} \in  \{0,\dots,k\} $ indicating the minimum number of clauses required to be  true  to satisfy a formula, and
	\item a data-fidelity  parameter $ \lambda \in \mathbb{R}^+ $,
\end{enumerate}
we  learn  a classification rule $ \Rule $ expressed as a $ k $ clause relaxed-CNF formula separating samples of class $ 1 $ from class $ 0 $. 



%We now present the formal definition of the problem. Given  (i) an instance space   $ (\mathbf{X}, \mathbf{y})$, where $ \mathbf{X} \in \{0,1\}^{n\times m} $ is the feature matrix with $ m $ binary features and $ n $ samples,\footnote{$\mathbf{X}$ contains both the features and their complements as columns as in~\cite{MV2013}.} and $ \mathbf{y} \in \{0,1\}^n $ is the class label vector, (ii) {a positive integer $ \noclause $ indicating the number of clauses in the desired rule}, (iii)  an integer threshold on literals $ \card_{l} \in  \{0,\dots,m\} $ indicating  the  minimum number of literals required to be true  to satisfy a clause, (iv)  an  integer threshold on clauses $ \card_{c} \in  \{0,\dots,k\} $ indicating the minimum number of clauses required to be  true  to satisfy a formula, and  (v) the data-fidelity  parameter $ \lambda \in [0,1]$, we  learn  a classification rule $ \Rule $, that is expressed as a $ k $ clause relaxed-CNF formula. 

% \[\hat{y}=\Rule(\mathbf{x})=I[\sum_{x_i\in\Rule, \Rule \subseteq \mathbf{x} } x_i \ge k]\]

Our goal is to find rules that balance two 
goals:   being accurate while also sparse to avoid over-fitting.  
To this end, we seek to minimize the total number of literals in all clauses, which motivates us to  find $ \Rule  $ with minimum  $ |\Rule| $. In particular, suppose $ \Rule $ classifies all samples correctly, i.e., $ \forall i, y^{(i)}=\Rule(\mathbf{x}^{(i)}) $. Among all the rules that classify all samples correctly,  we choose the sparsest  such $ \Rule $: 


%\vspace{-10pt}
\[
\min\limits_{\mathcal{R}} |\mathcal{R}|\text{ such that }\forall i, y^{(i)}=\Rule(\mathbf{x}^{(i)})
\]
%\vspace{-10pt}

% $ |\mathcal{R}|=\sum_{i=1}^k |C_i (\Rule)| $ is the  total number of literals appearing in $ \mathcal{R} $. 


In practical classification tasks, perfect classification is very unusual. Hence, we need to balance rule-sparsity with prediction error.  Let $ \mathcal{E}_{\dataset} $   be  the set of samples which are misclassified  by $ \mathcal{R} $ on the dataset $ \dataset $, i.e., $ \mathcal{E}_\dataset = \{(\mathbf{x}^{(i)},y^{(i)}) | y^{(i)} \ne \mathcal{R}(\mathbf{x}^{(i)}) \} $. Hence we aim to find $ \mathcal{R} $ as follows:\footnote{In our formulation, it is  straightforward to add class-conditional weights  (e.g. to penalize  false-alarms more than mis-detects), and to allow instance weights (per sample).}

%\vspace{-12pt}
\[
\min\limits_{\mathcal{R}} \;  \frac{\lambda}{n}|\mathcal{E}_{\dataset}| + \frac{1-\lambda}{k\cdot m} |\mathcal{R}|  
\]
%\vspace{-13pt}

Each term in the  objective function is normalized in $ [0,1] $. 
The data-fidelity parameter $ \lambda \in [0,1] $  serves to balance
the trade-off between prediction accuracy and sparsity. Higher 
values of $ \lambda $ produce lower prediction errors by sacrificing the sparsity 
of $ \Rule $, and vice versa. It can be viewed as an inverse of the regularization 
parameter. 