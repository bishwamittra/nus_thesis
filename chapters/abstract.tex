\begin{abstract}
	
	The significant success of machine learning in past decades has witnessed a host of applications of algorithmic decision-making in different safety-critical domains. The high-stake predictions of machine learning in medical, law, education, transportation and so on have far-reaching consequences on the end-users. Consequently, researchers call for the regulation of machine learning by defining and improving the interpretability, fairness, robustness, and privacy of predictions.  In this thesis, we focus on the interpretability and fairness aspects of machine learning, particularly on \emph{learning interpretable rule-based classifiers}, \emph{verifying fairness}, and \emph{identifying the sources of unfairness.} Prior studies aimed for these problems are limited by either scalability or  accuracy or both. To alleviate these limitations, we apply formal methods in interpretable and fair machine learning and provide scalable and accurate solutions to the underlying problems.
	
	
	In interpretable machine learning, rule-based classifiers are particularly effective in representing the decision boundary using a set of rules. The interpretability of rule-based classifiers is generally related to the size of the rules, where smaller rules with higher accuracy are preferable in practice. As such, interpretable classification learning becomes a combinatorial optimization problem suffering from poor scalability in large datasets. To this end, we propose an incremental learning framework, called {\imli}, which extends the classification problem to million-size datasets through an iterative solving of MaxSAT (maximum satisfiability) queries in mini-batch learning. Building on incremental learning and MILP (mixed integer linear programming) solving, we propose another interpretable learning framework, called {\crr}, for learning a more expressible classification rule by relaxing logical formulas, obtaining higher accuracy and less rule-size than existing interpretable classifiers.
	
	
	Fairness in machine learning centers on quantifying and mitigating the bias or unfairness of machine learning. In the presence of multiple fairness definitions and fairness algorithms, we propose a probabilistic fairness verifier, called {\justicia}, based on SSAT (stochastic satisfiability) with the goal of verifying whether the classifier achieves the desired level of fairness given the distribution of features. We also extend {\justicia} to consider feature correlations represented as a Bayesian Network, resulting in an accurate and scalable verification of fairness. Finally, we propose a framework for fairness influence functions (FIF) with an aim of quantifying the influence of features as their contribution on the bias of a classifier. FIF analysis interprets fairness by revealing potential features or subset of features attributing highly to the bias. Building on global sensitivity analysis, we propose an algorithm, called {\fairXplainer}, for computing FIFs of features, resulting in an accurate estimate of the bias of the classifier decomposed to input features. 
	
	

\end{abstract}