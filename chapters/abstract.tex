\begin{abstract}
	
	The significant success of machine learning in past decades has witnessed a host of applications of algorithmic decision-making in different safety-critical domains. The high-stake predictions of machine learning in medical, law, education, transportation and so on have far-reaching consequences on the end-users. Consequently, researchers call for the regulation of machine learning by defining and improving the interpretability, fairness, robustness, and privacy of predictions.  In this thesis, we focus on the interpretability and fairness aspects of machine learning, particularly on \emph{learning interpretable rule-based classifiers}, \emph{verifying fairness}, and \emph{explaining the sources of unfairness.} Prior studies aimed for these problems are limited by either scalability or  accuracy or both. To alleviate these limitations, we apply formal methods in interpretable and fair machine learning and provide scalable and accurate solutions to the underlying problems.
	
	
	In interpretable machine learning, rule-based classifiers are particularly effective in representing the decision boundary using a set of rules. The interpretability of rule-based classifiers is generally related to the size of the rules, where smaller rules with higher accuracy are preferable in practice. As such, interpretable classification learning becomes a combinatorial optimization problem suffering from poor scalability in large datasets. To this end, we propose an incremental learning framework, called {\imli}, which extends the classification problem to million-size datasets through an iterative solving of MaxSAT (maximum satisfiability) queries in mini-batch learning. \blue{Furthermore, rule-based classifiers suffer from limited expressiveness due to being confined to propositional logic. To learn more expressible yet interpretable classification rules, we propose a relaxation of rule-based classifiers based on logical formulas and  an efficient learning framework, called {\crr},  building on incremental learning and MILP (mixed integer linear programming). {\crr} obtains higher accuracy yet less rule-size than existing interpretable classifiers.}
	
	
	\blue{Fairness in machine learning centers on quantifying and mitigating the bias or unfairness of machine learning classifiers. In the presence of multiple fairness metrics for quantifying bias, we propose a probabilistic fairness verifier, called {\justicia}, based on SSAT (stochastic satisfiability) with the goal of formally verifying the bias of a classifier given the probability distribution of features. Additionally, we extend {\justicia} to consider feature correlations represented as a Bayesian Network, resulting in an accurate and scalable verification of fairness. Fairness metrics globally quantify bias, but do not detect or explain its sources. To explain group-based fairness metrics, we propose fairness influence functions (FIF) with an aim of quantifying the influence of individual features and the intersection of multiple features  on the bias of a classifier. FIF  explains fairness by revealing potential individual or intersectional features attributing highly to the bias. Building on global sensitivity analysis, we propose an algorithm, called {\fairXplainer}, for estimating the FIFs of features, resulting in a better approximation of bias based on FIFs and a higher correlation of FIFs with fairness interventions.}
	
	

\end{abstract}